{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c510da1",
   "metadata": {},
   "source": [
    "# Training NN-model for tabular data\n",
    "In this notebook we train an NN-model using the similar approach as in the reference paper, i.e., we consider only tabular satellite data and not actual images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb088da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as K\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f50fb1d",
   "metadata": {},
   "source": [
    "We exclide olc_id = 5G55HPG4+MM7 since no Sentinel-2 data available. For how the file Target_and_input.csv (input from OpenLandMap) was created take a look at Soil_Data_Preparation_v0.2.R. Warnning message is not relevant for us since later we will drop the column casing this warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d665d3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sprin\\anaconda3\\envs\\ML\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(69068, 106)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_and_input_OpenLandMap = pd.read_csv(\"Target_and_input.csv\")\n",
    "target_and_input_OpenLandMap = target_and_input_OpenLandMap[target_and_input_OpenLandMap.olc_id != \"5G55HPG4+MM7\"]\n",
    "target_and_input_OpenLandMap.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd78c335",
   "metadata": {},
   "source": [
    "Now, also get data from Sentinel-2. For how the file pixels_df.csv was created take a look at Download_Sentinel_2_Pixels.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd34bad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15478, 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_Sentinel_2 = pd.read_csv(\"pixels_df.csv\")\n",
    "input_Sentinel_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11ab3b8",
   "metadata": {},
   "source": [
    "Merge OpenLandMap and Sentinel-2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71f689a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69068, 119)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = target_and_input_OpenLandMap.merge(input_Sentinel_2, on='olc_id', how='left')\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3147b6",
   "metadata": {},
   "source": [
    "Since we want to predict organic carbon (oc), we keep only those table rows where oc is not NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ee65ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.dropna(subset=['oc'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f6ac28",
   "metadata": {},
   "source": [
    "Now, drop some columns that we do not need. For definition of these columns, take a look at Soil_Data_Preparation_v0.2.R.\n",
    "To summarize: we keep only:\n",
    " - Target variable (oc)\n",
    " - Measurements depth\n",
    " - Geo-coordinates (longitude and latitude)\n",
    " - Data from OpenLandMap\n",
    " - Data from Sentinel-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a37376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.drop(['olc_id','confidence_degree','uuid','site_obsdate','source_db','layer_sequence.f','hzn_top', 'hzn_bot', 'n_tot', 'ph_h2o'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cdb82fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['longitude_decimal_degrees', 'latitude_decimal_degrees', 'hzn_depth',\n",
       "       'oc', 'clm_precipitation_sm2rain.apr_m_1km_s0..0cm_2007..2018_v0.2.tif',\n",
       "       'clm_precipitation_sm2rain.aug_m_1km_s0..0cm_2007..2018_v0.2.tif',\n",
       "       'clm_precipitation_sm2rain.dec_m_1km_s0..0cm_2007..2018_v0.2.tif',\n",
       "       'clm_precipitation_sm2rain.feb_m_1km_s0..0cm_2007..2018_v0.2.tif',\n",
       "       'clm_precipitation_sm2rain.jan_m_1km_s0..0cm_2007..2018_v0.2.tif',\n",
       "       'clm_precipitation_sm2rain.jul_m_1km_s0..0cm_2007..2018_v0.2.tif',\n",
       "       ...\n",
       "       'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B10', 'B11', 'B12'],\n",
       "      dtype='object', length=109)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef5d7c",
   "metadata": {},
   "source": [
    "Let's take a look at distribution of oc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "520fff2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(all_data['oc'], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68ad363",
   "metadata": {},
   "source": [
    "We drop all measurements above (or equal to) 120 for following reasons:\n",
    "\n",
    "- It is only 193 measurement points (0.344 %), so it might be anomalies (#sum(all_data['oc']>=120)/all_data['oc'].shape[0]*100)\n",
    "- typical soils are between 5 and 30. Soils with organic carbon > 120-180 are considered in general as organic soils, also called histol. Histosol seems to be very rare in Africa and is not suitable for cultivation, see: https://en.wikipedia.org/wiki/Histosol and https://www.researchgate.net/figure/2-Distribution-of-Histosols-in-Africa-at-continental-scale-after-Koohafkan-et-al_fig3_323624025\n",
    "- In the reference paper (https://www.nature.com/articles/s41598-021-85639-y#Tab1) in Fig.4, the authors seem to not bother about values above 200.\n",
    "- iSDAsoil (https://www.isda-africa.com/isdasoil/) is based on the reference paper and seems to not trusts its own predictions above 40. \n",
    "\n",
    "Altoghether: exclusion of vcalues above 120 seems to be a reasonable compromise and do not affect the future \"every-day\"-\n",
    "application of our system since (probably) nobody will want to plant anything in histosol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d63091f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data[all_data['oc'] < 120]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3149f9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(all_data['oc'], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ace068",
   "metadata": {},
   "source": [
    "Split the data into train, validate, and test data sets with controlled seed.\n",
    "\n",
    "But before, here some remaks on training strategy:\n",
    "We randomely split the data set into Training, Development, and Test data sets.\n",
    "1. Training data (60% of all data): are used to train the model\n",
    "2. Validation data (20% of all data): are evaluated during the training but are not used for training. Observing evolution of optimization metric on this data set allows prevent overfitting.\n",
    "3. Test data (20% of all data): Data never seen by an algorithm. This data is used to evaluate the quality of the algorithm.\n",
    "\n",
    "After a model is trained and relevant metrics set are calculated on test data, we also need to know how much we can trust these results. The reason for potential mistrust is that even though the splitting of data into training/validation/test data sets is made randomly, a particular choice of training data might have impact on the accuracy metrics. It means that applied on completely new data, the model would produce predictions of different accuracy. To take this effect into account, we will repeat the random splitting 10 times (with different seeds). Then the model is trained from scratch without changing any of hyperparameters. The resulting metrics are then combined with the original metrics. So, we have 11 measurements for each relevant metric that have some particular distribution. This distribution is much better indicator of model quality as only one measurements of a metric. This procedure is basically equivalent to nested 2-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca69c3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = \\\n",
    "              np.split(all_data.sample(frac=1, random_state=42), \n",
    "                       [int(.6*len(all_data)), int(.8*len(all_data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbc26804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55885, 109), (33531, 109), (11177, 109), (11177, 109))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape, train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f92d3c",
   "metadata": {},
   "source": [
    "Some of the innputs from OpenLandMap and Sentinel-2 images have NAs. For the very first MVp, we simply set NAs to zero. It might not be an ideal choise, especially for data like temperature. We need to invest more time in better dealing with NAs in the future. Also, maybe it is better/easier to just drop all NAs (we will lose over 2k data points, but we will still have 53.462) For now, only some statistics on NAs (variable is mentioned only if it has NAs: \n",
    "\n",
    "- dtm_landform_usgs.ecotapestry_c_250m_s0..0cm_2014_v1.0.tif: 177\n",
    "- dtm_lithology_usgs.ecotapestry_c_250m_s0..0cm_2014_v1.0.tif: 177\n",
    "- clm_snow.prob_esacci.apr_p_1km_s0..0cm_2000..2016_v1.0.tif: 30\n",
    "- clm_snow.prob_esacci.aug_p_1km_s0..0cm_2000..2016_v1.0.tif: 125\n",
    "- clm_snow.prob_esacci.jul_p_1km_s0..0cm_2000..2016_v1.0.tif: 19\n",
    "- clm_snow.prob_esacci.jun_p_1km_s0..0cm_2000..2016_v1.0.tif: 31\n",
    "- clm_snow.prob_esacci.mar_p_1km_s0..0cm_2000..2016_v1.0.tif: 10\n",
    "- clm_snow.prob_esacci.may_p_1km_s0..0cm_2000..2016_v1.0.tif: 24\n",
    "- clm_snow.prob_esacci.oct_p_1km_s0..0cm_2000..2016_v1.0.tif: 19\n",
    "- clm_snow.prob_esacci.sep_p_1km_s0..0cm_2000..2016_v1.0.tif: 62\n",
    "- dtm_aspect-cosine_merit.dem_m_250m_s0..0cm_2018_v1.0.tif: 5\n",
    "- dtm_aspect-sine_merit.dem_m_250m_s0..0cm_2018_v1.0.tif: 5\n",
    "- dtm_convergence_merit.dem_m_250m_s0..0cm_2018_v1.0.tif: 5\n",
    "- dtm_cti_merit.dem_m_250m_s0..0cm_2018_v1.0.tif: 5\n",
    "- dtm_dev-magnitude_merit.dem_m_250m_s0..0cm_2018_v1.0.tif: 5\n",
    "- dtm_dev-scale_merit.dem_m_250m_s0..0cm_2018_v1.0.tif: 5\n",
    "- dtm_easthness_merit.dem_m_250m_s0..0cm_2018_v1.0.tif: 5\n",
    "- dtm_geom_merit.dem_m_250m_s0..0cm_2018_v1.0.tif: 5\n",
    "- dtm_northness_merit.dem_m_250m_s0..0cm_2018_v1.0.tif: 5\n",
    "- dtm_pcurv_merit.dem_m_250m_s0..0cm_2018_v1.0.tif: 5\n",
    "- dtm_rough-magnitude_merit.dem_m_250m_s0..0cm_2018_v1.0.tif: 5\n",
    "- dtm_rough-scale_merit.dem_m_250m_s0..0cm_2018_v1.0.tif: 5\n",
    "- dtm_roughness_merit.dem_m_250m_s0..0cm_2018_v1.0.tif: 5\n",
    "- dtm_tcurv_merit.dem_m_250m_s0..0cm_2018_v1.0.tif: 5\n",
    "- dtm_vrm_merit.dem_m_250m_s0..0cm_2018_v1.0.tif: 5\n",
    "- clm_lst_mod11a2.apr.day_m_1km_s0..0cm_2000..2017_v1.0.tif: 101\n",
    "- clm_lst_mod11a2.apr.day_sd_1km_s0..0cm_2000..2017_v1.0.tif: 101\n",
    "- clm_lst_mod11a2.apr.daynight_m_1km_s0..0cm_2000..2017_v1.0.tif: 112\n",
    "- clm_lst_mod11a2.aug.day_m_1km_s0..0cm_2000..2017_v1.0.tif: 347\n",
    "- clm_lst_mod11a2.aug.day_sd_1km_s0..0cm_2000..2017_v1.0.tif: 939\n",
    "- clm_lst_mod11a2.aug.daynight_m_1km_s0..0cm_2000..2017_v1.0.tif: 1159\n",
    "- clm_lst_mod11a2.dec.day_m_1km_s0..0cm_2000..2017_v1.0.tif: 101\n",
    "- clm_lst_mod11a2.dec.day_sd_1km_s0..0cm_2000..2017_v1.0.tif: 101\n",
    "- clm_lst_mod11a2.dec.daynight_m_1km_s0..0cm_2000..2017_v1.0.tif: 141\n",
    "- clm_lst_mod11a2.feb.day_m_1km_s0..0cm_2000..2017_v1.0.tif: 101\n",
    "- clm_lst_mod11a2.feb.day_sd_1km_s0..0cm_2000..2017_v1.0.tif: 101\n",
    "- clm_lst_mod11a2.feb.daynight_m_1km_s0..0cm_2000..2017_v1.0.tif: 101\n",
    "- clm_lst_mod11a2.jan.day_m_1km_s0..0cm_2000..2017_v1.0.tif: 101\n",
    "- clm_lst_mod11a2.jan.day_sd_1km_s0..0cm_2000..2017_v1.0.tif: 101\n",
    "- clm_lst_mod11a2.jan.daynight_m_1km_s0..0cm_2000..2017_v1.0.tif: 101\n",
    "- clm_lst_mod11a2.jul.day_m_1km_s0..0cm_2000..2017_v1.0.tif: 130\n",
    "- clm_lst_mod11a2.jul.day_sd_1km_s0..0cm_2000..2017_v1.0.tif: 202\n",
    "- clm_lst_mod11a2.jul.daynight_m_1km_s0..0cm_2000..2017_v1.0.tif: 432\n",
    "- clm_lst_mod11a2.jun.day_m_1km_s0..0cm_2000..2017_v1.0.tif: 101\n",
    "- clm_lst_mod11a2.jun.day_sd_1km_s0..0cm_2000..2017_v1.0.tif: 106\n",
    "- clm_lst_mod11a2.jun.daynight_m_1km_s0..0cm_2000..2017_v1.0.tif: 153\n",
    "- clm_lst_mod11a2.mar.day_m_1km_s0..0cm_2000..2017_v1.0.tif: 101\n",
    "- clm_lst_mod11a2.mar.day_sd_1km_s0..0cm_2000..2017_v1.0.tif: 101\n",
    "- clm_lst_mod11a2.mar.daynight_m_1km_s0..0cm_2000..2017_v1.0.tif: 101\n",
    "- clm_lst_mod11a2.may.day_m_1km_s0..0cm_2000..2017_v1.0.tif: 101\n",
    "- clm_lst_mod11a2.may.day_sd_1km_s0..0cm_2000..2017_v1.0.tif: 101\n",
    "- clm_lst_mod11a2.may.daynight_m_1km_s0..0cm_2000..2017_v1.0.tif: 105\n",
    "- clm_lst_mod11a2.nov.day_m_1km_s0..0cm_2000..2017_v1.0.tif: 106\n",
    "- clm_lst_mod11a2.nov.day_sd_1km_s0..0cm_2000..2017_v1.0.tif: 106\n",
    "- clm_lst_mod11a2.nov.daynight_m_1km_s0..0cm_2000..2017_v1.0.tif: 213\n",
    "- clm_lst_mod11a2.oct.day_m_1km_s0..0cm_2000..2017_v1.0.tif: 181\n",
    "- clm_lst_mod11a2.oct.day_sd_1km_s0..0cm_2000..2017_v1.0.tif: 291\n",
    "- clm_lst_mod11a2.oct.daynight_m_1km_s0..0cm_2000..2017_v1.0.tif: 404\n",
    "- clm_lst_mod11a2.sep.day_m_1km_s0..0cm_2000..2017_v1.0.tif: 122\n",
    "- clm_lst_mod11a2.sep.day_sd_1km_s0..0cm_2000..2017_v1.0.tif: 230\n",
    "- clm_lst_mod11a2.sep.daynight_m_1km_s0..0cm_2000..2017_v1.0.tif: 630\n",
    "- veg_fapar_proba.v.aug_d_250m_s0..0cm_2014..2017_v1.0.tif: 28\n",
    "- veg_fapar_proba.v.jul_d_250m_s0..0cm_2014..2017_v1.0.tif: 16\n",
    "- veg_fapar_proba.v.sep_d_250m_s0..0cm_2014..2017_v1.0.tif: 21\n",
    "- B01: 6\n",
    "- B02: 6\n",
    "- B03: 6\n",
    "- B04: 6\n",
    "- B05: 6\n",
    "- B06: 6\n",
    "- B07: 6\n",
    "- B08: 6\n",
    "- B8A: 6\n",
    "- B09: 6\n",
    "- B10: 6\n",
    "- B11: 6\n",
    "- B12: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0d813c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = train[\"oc\"]\n",
    "validate_targets = validate[\"oc\"]\n",
    "test_targets = test[\"oc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f94128ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = train.drop(['oc'], axis=1)\n",
    "validate_inputs = validate.drop(['oc'], axis=1)\n",
    "test_inputs = test.drop(['oc'], axis=1)\n",
    "\n",
    "train_inputs.fillna(0, inplace = True)\n",
    "validate_inputs.fillna(0, inplace = True)\n",
    "test_inputs.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01fb576",
   "metadata": {},
   "source": [
    "We use min-max nbormalization since it is common for images (even though, currently we use only tabular input). It means all input data is normalized to 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c140bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_values = train_inputs.min(axis=0)\n",
    "max_values = train_inputs.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "827c36c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_values.to_csv(\"min_values.csv\",index=False)\n",
    "max_values.to_csv(\"max_values.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7dc005ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs=(train_inputs-min_values)/(max_values-min_values)\n",
    "validate_inputs=(validate_inputs-min_values)/(max_values-min_values)\n",
    "test_inputs=(test_inputs-min_values)/(max_values-min_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ffea47",
   "metadata": {},
   "source": [
    "Important assumption: to derive actions for farmers based on soil quality factors, there is a need for precise predictions of organic carbon at lower scales (where it might decide about life and death). At higher scales, however, precision is not so important (there is for sure enough oc in the soil). So, we should bias the model towards better presision at lower scales. Therefore, instead of predicting oc, we should predict logarithm of it. To also be able to predict oc = 0, we should use transformation ln(oc+1). In the reference paper, the authors do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "271f3266",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = np.log(train_targets+1)\n",
    "validate_targets = np.log(validate_targets+1)\n",
    "test_targets = np.log(test_targets+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0e2653e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = train_inputs.to_numpy()\n",
    "validate_inputs = validate_inputs.to_numpy()\n",
    "test_inputs = test_inputs.to_numpy()\n",
    "\n",
    "train_targets = train_targets.to_numpy()\n",
    "validate_targets = validate_targets.to_numpy()\n",
    "test_targets = test_targets.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0784179f",
   "metadata": {},
   "source": [
    "Below, the actual model is defined. It is a quite simple Deep NN with batch normalization. The latter is good accelerating learning process and also has some moderate regulirization effect. In the first experiments, I did not observe the need for more regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10bee927",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = K.models.Sequential()\n",
    "model.add(K.Input(shape=(108,)))\n",
    "model.add(K.layers.Dense(512, activation='relu'))\n",
    "#model.add(K.layers.Dropout(0.05))\n",
    "model.add(K.layers.BatchNormalization())\n",
    "model.add(K.layers.Dense(256, activation='relu'))\n",
    "#model.add(K.layers.Dropout(0.05))\n",
    "model.add(K.layers.BatchNormalization())\n",
    "model.add(K.layers.Dense(128, activation='relu'))\n",
    "#model.add(K.layers.Dropout(0.05))\n",
    "model.add(K.layers.BatchNormalization())\n",
    "model.add(K.layers.Dense(64, activation='relu'))\n",
    "#model.add(K.layers.Dropout(0.05))\n",
    "model.add(K.layers.BatchNormalization())\n",
    "model.add(K.layers.Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74734378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               55808     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 232,193\n",
      "Trainable params: 230,273\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e41e6c",
   "metadata": {},
   "source": [
    "We use Early Stopping on validation data set with patience=30. I.e., if the optimization function (in this case mse: mean-square-error) on validation data set does not improve for 30 epochs, the training process is stopped and the model coefficients that produced the best mse for validation data set are restored.\n",
    "Max number of epoches is 30 and batch sized used is 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53c0d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_siz = 32\n",
    "num_epochs = 300\n",
    "callback = K.callbacks.EarlyStopping(monitor='val_loss', patience=30, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1b221fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse',\n",
    "              optimizer=K.optimizers.Adam(learning_rate=0.0001)\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10205d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1048/1048 [==============================] - 6s 4ms/step - loss: 2.4840 - val_loss: 0.9478\n",
      "Epoch 2/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.5832 - val_loss: 0.3660\n",
      "Epoch 3/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3815 - val_loss: 0.3706\n",
      "Epoch 4/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3502 - val_loss: 0.3365\n",
      "Epoch 5/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3318 - val_loss: 0.3029\n",
      "Epoch 6/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3204 - val_loss: 0.3167\n",
      "Epoch 7/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3074 - val_loss: 0.3008\n",
      "Epoch 8/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2979 - val_loss: 0.2939\n",
      "Epoch 9/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2934 - val_loss: 0.2815\n",
      "Epoch 10/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2890 - val_loss: 0.2701\n",
      "Epoch 11/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2794 - val_loss: 0.2667\n",
      "Epoch 12/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2766 - val_loss: 0.2842\n",
      "Epoch 13/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2675 - val_loss: 0.2641\n",
      "Epoch 14/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2629 - val_loss: 0.2742\n",
      "Epoch 15/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2574 - val_loss: 0.2582\n",
      "Epoch 16/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2553 - val_loss: 0.2602\n",
      "Epoch 17/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2536 - val_loss: 0.2757\n",
      "Epoch 18/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2501 - val_loss: 0.2767\n",
      "Epoch 19/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2491 - val_loss: 0.2650\n",
      "Epoch 20/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2413 - val_loss: 0.2526\n",
      "Epoch 21/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2371 - val_loss: 0.2501\n",
      "Epoch 22/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2349 - val_loss: 0.2440\n",
      "Epoch 23/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2320 - val_loss: 0.2421\n",
      "Epoch 24/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2316 - val_loss: 0.2650\n",
      "Epoch 25/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2273 - val_loss: 0.2383\n",
      "Epoch 26/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2235 - val_loss: 0.2646\n",
      "Epoch 27/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2226 - val_loss: 0.2412\n",
      "Epoch 28/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2190 - val_loss: 0.2466\n",
      "Epoch 29/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2206 - val_loss: 0.2331\n",
      "Epoch 30/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2181 - val_loss: 0.2338\n",
      "Epoch 31/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2144 - val_loss: 0.2555\n",
      "Epoch 32/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2129 - val_loss: 0.2340\n",
      "Epoch 33/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2102 - val_loss: 0.2248\n",
      "Epoch 34/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2083 - val_loss: 0.2492\n",
      "Epoch 35/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2073 - val_loss: 0.2339\n",
      "Epoch 36/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2060 - val_loss: 0.2419\n",
      "Epoch 37/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2046 - val_loss: 0.2285\n",
      "Epoch 38/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2018 - val_loss: 0.2274\n",
      "Epoch 39/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1999 - val_loss: 0.2382\n",
      "Epoch 40/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2007 - val_loss: 0.2292\n",
      "Epoch 41/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1968 - val_loss: 0.2349\n",
      "Epoch 42/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1951 - val_loss: 0.2350\n",
      "Epoch 43/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1932 - val_loss: 0.2275\n",
      "Epoch 44/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1938 - val_loss: 0.2261\n",
      "Epoch 45/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1926 - val_loss: 0.2477\n",
      "Epoch 46/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1886 - val_loss: 0.2240\n",
      "Epoch 47/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1875 - val_loss: 0.2286\n",
      "Epoch 48/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1883 - val_loss: 0.2355\n",
      "Epoch 49/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1894 - val_loss: 0.2329\n",
      "Epoch 50/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1892 - val_loss: 0.2295\n",
      "Epoch 51/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1827 - val_loss: 0.2339\n",
      "Epoch 52/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1832 - val_loss: 0.2238\n",
      "Epoch 53/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1827 - val_loss: 0.2245\n",
      "Epoch 54/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1824 - val_loss: 0.2203\n",
      "Epoch 55/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1800 - val_loss: 0.2297\n",
      "Epoch 56/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1769 - val_loss: 0.2173\n",
      "Epoch 57/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1777 - val_loss: 0.2164\n",
      "Epoch 58/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1783 - val_loss: 0.2179\n",
      "Epoch 59/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1744 - val_loss: 0.2458\n",
      "Epoch 60/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1755 - val_loss: 0.2228\n",
      "Epoch 61/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1736 - val_loss: 0.2195\n",
      "Epoch 62/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1727 - val_loss: 0.2297\n",
      "Epoch 63/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1743 - val_loss: 0.2143\n",
      "Epoch 64/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1732 - val_loss: 0.2271\n",
      "Epoch 65/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1694 - val_loss: 0.2199\n",
      "Epoch 66/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1674 - val_loss: 0.2128\n",
      "Epoch 67/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1696 - val_loss: 0.2421\n",
      "Epoch 68/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1668 - val_loss: 0.2163\n",
      "Epoch 69/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1668 - val_loss: 0.2193\n",
      "Epoch 70/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1641 - val_loss: 0.2166\n",
      "Epoch 71/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1650 - val_loss: 0.2151\n",
      "Epoch 72/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1650 - val_loss: 0.2254\n",
      "Epoch 73/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1664 - val_loss: 0.2121\n",
      "Epoch 74/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1614 - val_loss: 0.2258\n",
      "Epoch 75/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1628 - val_loss: 0.2138\n",
      "Epoch 76/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1627 - val_loss: 0.2201\n",
      "Epoch 77/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1620 - val_loss: 0.2105\n",
      "Epoch 78/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1615 - val_loss: 0.2084\n",
      "Epoch 79/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1607 - val_loss: 0.2130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1583 - val_loss: 0.2098\n",
      "Epoch 81/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1586 - val_loss: 0.2252\n",
      "Epoch 82/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1577 - val_loss: 0.2149\n",
      "Epoch 83/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1589 - val_loss: 0.2164\n",
      "Epoch 84/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1576 - val_loss: 0.2241\n",
      "Epoch 85/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1546 - val_loss: 0.2105\n",
      "Epoch 86/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1565 - val_loss: 0.2110\n",
      "Epoch 87/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1527 - val_loss: 0.2039\n",
      "Epoch 88/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1537 - val_loss: 0.2117\n",
      "Epoch 89/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1520 - val_loss: 0.2078\n",
      "Epoch 90/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1536 - val_loss: 0.2041\n",
      "Epoch 91/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1498 - val_loss: 0.2065\n",
      "Epoch 92/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1502 - val_loss: 0.2047\n",
      "Epoch 93/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1522 - val_loss: 0.2127\n",
      "Epoch 94/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1503 - val_loss: 0.2092\n",
      "Epoch 95/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1512 - val_loss: 0.2069\n",
      "Epoch 96/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1475 - val_loss: 0.2077\n",
      "Epoch 97/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1490 - val_loss: 0.2113\n",
      "Epoch 98/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1499 - val_loss: 0.2116\n",
      "Epoch 99/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1480 - val_loss: 0.2091\n",
      "Epoch 100/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1480 - val_loss: 0.2125\n",
      "Epoch 101/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1480 - val_loss: 0.2156\n",
      "Epoch 102/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1434 - val_loss: 0.2061\n",
      "Epoch 103/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1475 - val_loss: 0.2130\n",
      "Epoch 104/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1449 - val_loss: 0.2052\n",
      "Epoch 105/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1437 - val_loss: 0.2057\n",
      "Epoch 106/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1433 - val_loss: 0.2041\n",
      "Epoch 107/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1447 - val_loss: 0.2219\n",
      "Epoch 108/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1418 - val_loss: 0.2179\n",
      "Epoch 109/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1445 - val_loss: 0.2057\n",
      "Epoch 110/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1428 - val_loss: 0.2125\n",
      "Epoch 111/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1417 - val_loss: 0.2049\n",
      "Epoch 112/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1430 - val_loss: 0.1992\n",
      "Epoch 113/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1431 - val_loss: 0.2055\n",
      "Epoch 114/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1394 - val_loss: 0.2039\n",
      "Epoch 115/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1393 - val_loss: 0.2055\n",
      "Epoch 116/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1413 - val_loss: 0.2232\n",
      "Epoch 117/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1424 - val_loss: 0.2019\n",
      "Epoch 118/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1444 - val_loss: 0.2073\n",
      "Epoch 119/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1392 - val_loss: 0.2327\n",
      "Epoch 120/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1400 - val_loss: 0.2070\n",
      "Epoch 121/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1381 - val_loss: 0.2235\n",
      "Epoch 122/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1364 - val_loss: 0.2020\n",
      "Epoch 123/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1377 - val_loss: 0.2035\n",
      "Epoch 124/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1385 - val_loss: 0.2021\n",
      "Epoch 125/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1363 - val_loss: 0.2058\n",
      "Epoch 126/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1353 - val_loss: 0.2246\n",
      "Epoch 127/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1354 - val_loss: 0.2175\n",
      "Epoch 128/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1344 - val_loss: 0.2065\n",
      "Epoch 129/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1345 - val_loss: 0.2173\n",
      "Epoch 130/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1360 - val_loss: 0.2064\n",
      "Epoch 131/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1358 - val_loss: 0.2064\n",
      "Epoch 132/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1351 - val_loss: 0.2095\n",
      "Epoch 133/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1334 - val_loss: 0.2095\n",
      "Epoch 134/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1342 - val_loss: 0.2109\n",
      "Epoch 135/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1333 - val_loss: 0.2050\n",
      "Epoch 136/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1326 - val_loss: 0.2098\n",
      "Epoch 137/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1317 - val_loss: 0.2069\n",
      "Epoch 138/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1312 - val_loss: 0.2197\n",
      "Epoch 139/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1302 - val_loss: 0.2048\n",
      "Epoch 140/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1340 - val_loss: 0.2250\n",
      "Epoch 141/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1311 - val_loss: 0.1976\n",
      "Epoch 142/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1316 - val_loss: 0.2017\n",
      "Epoch 143/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1303 - val_loss: 0.2093\n",
      "Epoch 144/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1301 - val_loss: 0.2098\n",
      "Epoch 145/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1314 - val_loss: 0.2050\n",
      "Epoch 146/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1310 - val_loss: 0.2040\n",
      "Epoch 147/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1321 - val_loss: 0.2041\n",
      "Epoch 148/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1290 - val_loss: 0.1986\n",
      "Epoch 149/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1287 - val_loss: 0.2043\n",
      "Epoch 150/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1291 - val_loss: 0.2041\n",
      "Epoch 151/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1294 - val_loss: 0.1999\n",
      "Epoch 152/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1289 - val_loss: 0.2035\n",
      "Epoch 153/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1298 - val_loss: 0.2051\n",
      "Epoch 154/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1261 - val_loss: 0.2073\n",
      "Epoch 155/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1273 - val_loss: 0.1985\n",
      "Epoch 156/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1281 - val_loss: 0.2033\n",
      "Epoch 157/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1299 - val_loss: 0.2032\n",
      "Epoch 158/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1249 - val_loss: 0.2001\n",
      "Epoch 159/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1250 - val_loss: 0.2033\n",
      "Epoch 160/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1259 - val_loss: 0.2048\n",
      "Epoch 161/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1233 - val_loss: 0.1983\n",
      "Epoch 162/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1239 - val_loss: 0.2044\n",
      "Epoch 163/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1253 - val_loss: 0.2077\n",
      "Epoch 164/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1242 - val_loss: 0.2031\n",
      "Epoch 165/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1263 - val_loss: 0.2002\n",
      "Epoch 166/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1262 - val_loss: 0.2003\n",
      "Epoch 167/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1227 - val_loss: 0.2026\n",
      "Epoch 168/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1228 - val_loss: 0.2060\n",
      "Epoch 169/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1236 - val_loss: 0.2014\n",
      "Epoch 170/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1210 - val_loss: 0.2005\n",
      "Epoch 171/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1215 - val_loss: 0.2065\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_inputs,train_targets,\n",
    "                    validation_data=(validate_inputs,validate_targets),\n",
    "                    epochs=num_epochs,\n",
    "                    callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "469da62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('model_as_in_paper_with_sentinel_regression.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f46c51c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import matplotlib to plot training evolution.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#acc = history.history['accuracy']\n",
    "#val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "#plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "#plt.plot(epochs, val_acc, 'b--', label='Validation acc')\n",
    "#plt.title('Training and validation accuracy')\n",
    "#plt.legend()\n",
    "\n",
    "#plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r--', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4dbb8c",
   "metadata": {},
   "source": [
    "Finally, we can produce predictions. Thereby following should be considered:\n",
    " - every prediction < 0 must be set to zero. Otherwise oc also becomes negative and it does not make sence\n",
    " - we evaluate on all data sets for completeness: training, validation, and test\n",
    " - The major metric of interest is root mean squared error (rmse) since it is also used as metric in the reference paper\n",
    " - In addition, one could also redefine the problem as classification problem (instead of regression). As a kind of natural classes one can use rounded ln(oc+1). I.e.,\n",
    "    1.\tClass 0 corresponds to oc between 0 and 0.65\n",
    "    2.\tClass 1 corresponds to oc between 0.65 and 3.5\n",
    "    3.\tClass 2 corresponds to oc between 3.5 and 11.2\n",
    "    4.\tClass 3 corresponds to oc between 11.2 and 32.1\n",
    "    5.\tClass 4 corresponds to oc between 32.1 and 89.0\n",
    "    6.\tClass 5 corresponds to oc between 89.0 and 120\n",
    "    \n",
    "   The metric here is be the number of data points with correctly predicted classes divided by the number of data points (accuracy). Note that I tried to build classification models, but in my experiments, they produced not so good results as regression models (accuracy was used to compare the models). The reason might be that classification models do not recognize natural order of classes. Nonetheless, I still report also the accuracy as metric to provide a better feeling on quality of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05e0b7e",
   "metadata": {},
   "source": [
    "First, training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22585b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = pd.DataFrame(model.predict(train_inputs))\n",
    "train_df = pd.DataFrame(train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf699ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions.clip(lower=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d104c0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.290209\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.square(train_predictions - train_df).sum(axis=0)/train_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "696865fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.789836\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.round(train_predictions) == np.round(train_df)).sum(axis=0)/train_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a56cfd",
   "metadata": {},
   "source": [
    "Plot distribution of actual and predicted target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8268f6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAS6UlEQVR4nO3db4xdd33n8fenNuFfC3aS2WzWNrW1uFmFaLukI8dRVqjBi+NQhPOAokS7xGW99YOaNqWVaMw+sAqNWrRV3UZbsnJjF6fNJkQBFKsbSEfBFa1EQsYh5J9JMjUQj5XgaeyEUlRY028f3J/DxczYnnvHc2c875d0Ned8z++c+z2y7M+c3zn3OlWFJGlh+6lBNyBJGjzDQJJkGEiSDANJEoaBJAlYPOgGenXhhRfWypUrB92GJM0r+/fv/4eqGjq5Pm/DYOXKlYyOjg66DUmaV5J8a7K600SSJMNAkmQYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgSeIMwiDJ7iRHkjw5ybbfTlJJLmzrSXJrkrEkjye5vGvspiTPtdemrvovJHmi7XNrkszUyWme2Pf7P3pJGogzuTL4FLDh5GKSFcB64Pmu8rXA6vbaAtzWxp4PbAeuANYA25MsbfvcBvxq134/8V6SpLPrtGFQVV8Cjk6yaQfwEaD7/83cCNxRHQ8BS5JcDFwDjFTV0ao6BowAG9q2N1XVQ9X5/zfvAK7r64wkSdPW0z2DJBuBw1X1tZM2LQMOda2Pt9qp6uOT1Kd63y1JRpOMTkxM9NK6JGkS0/7W0iRvAD5KZ4poVlXVTmAnwPDwcJ1muOaJLx986dXlK68eYCPSAtbLlcG/B1YBX0vyTWA58GiSfwscBlZ0jV3eaqeqL5+kLkmaRdMOg6p6oqr+TVWtrKqVdKZ2Lq+qF4G9wI3tqaK1wCtV9QLwALA+ydJ243g98EDb9p0ka9tTRDcC983QuUmSztCZPFp6F/Bl4JIk40k2n2L4/cBBYAz4M+DXAKrqKPBx4JH2+lir0cbc3vb5e+DzvZ2KJKlXp71nUFU3nGb7yq7lArZOMW43sHuS+ihw2en6kCSdPX4CWZJkGEiSDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkiTMIgyS7kxxJ8mRX7X8l+XqSx5N8LsmSrm3bkowleSbJNV31Da02luTmrvqqJA+3+qeTnDeD5ydJOgNncmXwKWDDSbUR4LKq+o/As8A2gCSXAtcDb2v7fDLJoiSLgD8FrgUuBW5oYwE+AeyoqrcCx4DNfZ2RJGnaThsGVfUl4OhJtb+uquNt9SFgeVveCNxdVd+vqm8AY8Ca9hqrqoNV9QPgbmBjkgDvBO5t++8BruvvlCRJ0zUT9wz+O/D5trwMONS1bbzVpqpfALzcFSwn6pNKsiXJaJLRiYmJGWhdkgR9hkGS/wkcB+6cmXZOrap2VtVwVQ0PDQ3NxltK0oKwuNcdk/wK8B5gXVVVKx8GVnQNW95qTFF/CViSZHG7OugeL0maJT1dGSTZAHwEeG9Vfa9r017g+iSvTbIKWA18BXgEWN2eHDqPzk3mvS1E9gHva/tvAu7r7VQkSb06k0dL7wK+DFySZDzJZuB/Az8DjCR5LMn/Aaiqp4B7gKeBLwBbq+qH7bf+DwEPAAeAe9pYgN8BfivJGJ17CLtm9AwlSad12mmiqrphkvKU/2BX1S3ALZPU7wfun6R+kM7TRpKkAfETyJKk3m8gaw7Y9/s/Wr562+D6kDTveWUgSTIMJEmGgSQJw0CShGEgScIwkCTho6Xz2pcPvvTq8pVXD7ARSfOeVwaSJMNAkmQYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSOIMwSLI7yZEkT3bVzk8ykuS59nNpqyfJrUnGkjye5PKufTa18c8l2dRV/4UkT7R9bk2SmT5JSdKpncmVwaeADSfVbgYerKrVwINtHeBaYHV7bQFug054ANuBK4A1wPYTAdLG/GrXfie/lyTpLDttGFTVl4CjJ5U3Anva8h7guq76HdXxELAkycXANcBIVR2tqmPACLChbXtTVT1UVQXc0XUsSdIs6fWewUVV9UJbfhG4qC0vAw51jRtvtVPVxyepTyrJliSjSUYnJiZ6bF2SdLK+byC33+hrBno5k/faWVXDVTU8NDQ0G28pSQtCr2Hw7TbFQ/t5pNUPAyu6xi1vtVPVl09SlyTNol7DYC9w4omgTcB9XfUb21NFa4FX2nTSA8D6JEvbjeP1wANt23eSrG1PEd3YdSxJ0iw57X97meQu4BeBC5OM03kq6A+Ae5JsBr4FvL8Nvx94NzAGfA/4IEBVHU3yceCRNu5jVXXipvSv0Xli6fXA59tLkjSLThsGVXXDFJvWTTK2gK1THGc3sHuS+ihw2en6kCSdPX4CWZJkGEiSDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkiT7DIMmHkzyV5MkkdyV5XZJVSR5OMpbk00nOa2Nf29bH2vaVXcfZ1urPJLmmz3OSJE1Tz2GQZBnwG8BwVV0GLAKuBz4B7KiqtwLHgM1tl83AsVbf0caR5NK239uADcAnkyzqtS9J0vT1O020GHh9ksXAG4AXgHcC97bte4Dr2vLGtk7bvi5JWv3uqvp+VX0DGAPW9NmXJGkaeg6DqjoM/CHwPJ0QeAXYD7xcVcfbsHFgWVteBhxq+x5v4y/ork+yz49JsiXJaJLRiYmJXluXJJ2kn2mipXR+q18F/DvgjXSmec6aqtpZVcNVNTw0NHQ230qSFpR+pon+C/CNqpqoqv8PfBa4CljSpo0AlgOH2/JhYAVA2/5m4KXu+iT7SJJmQT9h8DywNskb2tz/OuBpYB/wvjZmE3BfW97b1mnbv1hV1erXt6eNVgGrga/00ZckaZoWn37I5Krq4ST3Ao8Cx4GvAjuB/wfcneT3Wm1X22UX8BdJxoCjdJ4goqqeSnIPnSA5Dmytqh/22pckafp6DgOAqtoObD+pfJBJngaqqn8GfnmK49wC3NJPL5Kk3vkJZEmSYSBJMgwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJPoMgyRLktyb5OtJDiS5Msn5SUaSPNd+Lm1jk+TWJGNJHk9yeddxNrXxzyXZ1O9JSZKmp98rgz8BvlBV/wH4eeAAcDPwYFWtBh5s6wDXAqvbawtwG0CS84HtwBXAGmD7iQCRJM2OnsMgyZuBdwC7AKrqB1X1MrAR2NOG7QGua8sbgTuq4yFgSZKLgWuAkao6WlXHgBFgQ699SZKmb3Ef+64CJoA/T/LzwH7gJuCiqnqhjXkRuKgtLwMOde0/3mpT1X9Cki10rip4y1ve0kfr0szbMfLsq8sfftfPDbATafr6mSZaDFwO3FZVbwf+iR9NCQFQVQVUH+/xY6pqZ1UNV9Xw0NDQTB1Wkha8fsJgHBivqofb+r10wuHbbfqH9vNI234YWNG1//JWm6ouSZolPYdBVb0IHEpySSutA54G9gInngjaBNzXlvcCN7anitYCr7TppAeA9UmWthvH61tNkjRL+rlnAPDrwJ1JzgMOAh+kEzD3JNkMfAt4fxt7P/BuYAz4XhtLVR1N8nHgkTbuY1V1tM++JEnT0FcYVNVjwPAkm9ZNMraArVMcZzewu59epLlg7fM7Owv7LoCrtw22GWka/ASyJMkwkCQZBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kSMxAGSRYl+WqSv2rrq5I8nGQsyaeTnNfqr23rY237yq5jbGv1Z5Jc029PkqTpmYkrg5uAA13rnwB2VNVbgWPA5lbfDBxr9R1tHEkuBa4H3gZsAD6ZZNEM9CVJOkN9hUGS5cAvAbe39QDvBO5tQ/YA17XljW2dtn1dG78RuLuqvl9V3wDGgDX99CVJmp5+rwz+GPgI8C9t/QLg5ao63tbHgWVteRlwCKBtf6WNf7U+yT4/JsmWJKNJRicmJvpsXZJ0Qs9hkOQ9wJGq2j+D/ZxSVe2squGqGh4aGpqtt5Wkc97iPva9CnhvkncDrwPeBPwJsCTJ4vbb/3LgcBt/GFgBjCdZDLwZeKmrfkL3PpKkWdDzlUFVbauq5VW1ks4N4C9W1X8F9gHva8M2Afe15b1tnbb9i1VVrX59e9poFbAa+EqvfUmSpq+fK4Op/A5wd5LfA74K7Gr1XcBfJBkDjtIJEKrqqST3AE8Dx4GtVfXDs9CXJGkKMxIGVfU3wN+05YNM8jRQVf0z8MtT7H8LcMtM9CJJmj4/gSxJMgwkSYaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkkQfYZBkRZJ9SZ5O8lSSm1r9/CQjSZ5rP5e2epLcmmQsyeNJLu861qY2/rkkm/o/LUnSdPRzZXAc+O2quhRYC2xNcilwM/BgVa0GHmzrANcCq9trC3AbdMID2A5cAawBtp8IEEnS7Og5DKrqhap6tC3/I3AAWAZsBPa0YXuA69ryRuCO6ngIWJLkYuAaYKSqjlbVMWAE2NBrX5Kk6ZuRewZJVgJvBx4GLqqqF9qmF4GL2vIy4FDXbuOtNlVdkjRL+g6DJD8NfAb4zar6Tve2qiqg+n2PrvfakmQ0yejExMRMHVaSFry+wiDJa+gEwZ1V9dlW/nab/qH9PNLqh4EVXbsvb7Wp6j+hqnZW1XBVDQ8NDfXTuiSpSz9PEwXYBRyoqj/q2rQXOPFE0Cbgvq76je2porXAK2066QFgfZKl7cbx+laTJM2SxX3sexXwAeCJJI+12keBPwDuSbIZ+Bbw/rbtfuDdwBjwPeCDAFV1NMnHgUfauI9V1dE++pIkTVPPYVBVfwdkis3rJhlfwNYpjrUb2N1rL5Kk/vgJZEmSYSBJMgwkSRgGkiQMA0kShoEkif4+ZzBv7Rh59tXlD7/r5wbYiTT3+PdjYfLKQJK0MK8MJJ3a2ud3dhb2XQBXbxtsM5oVXhlIkgwDSdICnSZ69RIYgD8cWB+SNFd4ZSBJMgwkSYaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJOZQGCTZkOSZJGNJbh50P5K0kMyJr6NIsgj4U+BdwDjwSJK9VfX0YDuTNF/tGHn2x7565srNfvXMqcyVK4M1wFhVHayqHwB3AxsH3JMkLRipqkH3QJL3ARuq6n+09Q8AV1TVh04atwXY0lYvAZ7p8S0vBP6hx33nmnPlXM6V8wDPZa46V86l3/P42aoaOrk4J6aJzlRV7QR2nnbgaSQZrarhGWhp4M6VczlXzgM8l7nqXDmXs3Uec2Wa6DCwomt9eatJkmbBXAmDR4DVSVYlOQ+4Htg74J4kacGYE9NEVXU8yYeAB4BFwO6qeuosvmXfU01zyLlyLufKeYDnMledK+dyVs5jTtxAliQN1lyZJpIkDZBhIElaWGFwLn3lRZLdSY4keXLQvfQjyYok+5I8neSpJDcNuqdeJXldkq8k+Vo7l98ddE/9SLIoyVeT/NWge+lHkm8meSLJY0lGB91PP5IsSXJvkq8nOZDkyhk79kK5Z9C+8uJZur7yArhhvn7lRZJ3AN8F7qiqywbdT6+SXAxcXFWPJvkZYD9w3Xz8c0kS4I1V9d0krwH+Dripqh4acGs9SfJbwDDwpqp6z6D76VWSbwLDVTXvP3CWZA/wt1V1e3vy8g1V9fJMHHshXRmcU195UVVfAo4Ouo9+VdULVfVoW/5H4ACwbLBd9aY6vttWX9Ne8/K3rSTLgV8Cbh90L+pI8mbgHcAugKr6wUwFASysMFgGHOpaH2ee/qNzrkqyEng78PCAW+lZm1p5DDgCjFTVfD2XPwY+AvzLgPuYCQX8dZL97Stt5qtVwATw52367vYkb5ypgy+kMNAcluSngc8Av1lV3xl0P72qqh9W1X+i8yn6NUnm3RRekvcAR6pq/6B7mSH/uaouB64FtrYp1vloMXA5cFtVvR34J2DG7n0upDDwKy/mqDa//hngzqr67KD7mQnt8n0fsGHArfTiKuC9ba79buCdSf5ysC31rqoOt59HgM/RmTKej8aB8a6rzXvphMOMWEhh4FdezEHtpusu4EBV/dGg++lHkqEkS9ry6+k8rPD1gTbVg6raVlXLq2olnb8nX6yq/zbgtnqS5I3twQTalMp6YF4+gVdVLwKHklzSSuuAGXvQYk58HcVsGMBXXpxVSe4CfhG4MMk4sL2qdg22q55cBXwAeKLNtQN8tKruH1xLPbsY2NOeXPsp4J6qmtePZZ4DLgI+1/mdg8XA/62qLwy2pb78OnBn+4X2IPDBmTrwgnm0VJI0tYU0TSRJmoJhIEkyDCRJhoEkCcNAkoRhIEnCMJAkAf8KWTy4LqohTnkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(np.round(train_df), bins='auto', alpha=0.5)\n",
    "_ = plt.hist(np.round(train_predictions), bins='auto', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c102619",
   "metadata": {},
   "source": [
    "Now, validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81653252",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_predictions = pd.DataFrame(model.predict(validate_inputs))\n",
    "validate_df = pd.DataFrame(validate_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1946d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_predictions.clip(lower=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be163b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.444209\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.square(validate_predictions - validate_df).sum(axis=0)/validate_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c44cda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.692494\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.round(validate_predictions) == np.round(validate_df)).sum(axis=0)/validate_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a430895b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(np.round(validate_df), bins='auto', alpha=0.5)  # arguments are passed to np.histogram\n",
    "_ = plt.hist(np.round(validate_predictions), bins='auto', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883fee8e",
   "metadata": {},
   "source": [
    "And finally testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b25a8f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions =pd.DataFrame(model.predict(test_inputs))\n",
    "test_df = pd.DataFrame(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ef1c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions.clip(lower=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "758683e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.439565\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.square(test_predictions - test_df).sum(axis=0)/test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffe235d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.699472\n",
       "dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.round(test_predictions) == np.round(test_df)).sum(axis=0)/test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6639dafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(np.round(test_df), bins='auto', alpha=0.5)  # arguments are passed to np.histogram\n",
    "_ = plt.hist(np.round(test_predictions), bins='auto', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0208392",
   "metadata": {},
   "source": [
    "## Summary for one model\n",
    " - RMSE\n",
    "   - Train: 0.290209\n",
    "   - Valid: 0.444209\n",
    "   - Test: 0.439565\n",
    "\n",
    " - Accuracy (rounded in classes)\n",
    "   - Train: 0.789836\n",
    "   - Valid: 0.692494\n",
    "   - Test: 0.699472"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b35df4b",
   "metadata": {},
   "source": [
    "## Error Estimation\n",
    "Now, as already mentioned above we repeat the same process 10 times to estimate the error of our metrices. Thereby, different seeds are used to split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2debb288",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [24,3,123,7,666,5,10,1993,13,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a9401dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 2.4583 - val_loss: 1.4420\n",
      "Epoch 2/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.5832 - val_loss: 0.3650\n",
      "Epoch 3/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3783 - val_loss: 0.3494\n",
      "Epoch 4/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3436 - val_loss: 0.3127\n",
      "Epoch 5/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3274 - val_loss: 0.3047\n",
      "Epoch 6/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3108 - val_loss: 0.2843\n",
      "Epoch 7/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.3052 - val_loss: 0.2861\n",
      "Epoch 8/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2947 - val_loss: 0.2947\n",
      "Epoch 9/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2897 - val_loss: 0.2797\n",
      "Epoch 10/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2786 - val_loss: 0.3687\n",
      "Epoch 11/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2743 - val_loss: 0.2967\n",
      "Epoch 12/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2693 - val_loss: 0.2739\n",
      "Epoch 13/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2648 - val_loss: 0.2635\n",
      "Epoch 14/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2614 - val_loss: 0.2779\n",
      "Epoch 15/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2573 - val_loss: 0.2540\n",
      "Epoch 16/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2494 - val_loss: 0.2758\n",
      "Epoch 17/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2473 - val_loss: 0.2536\n",
      "Epoch 18/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2462 - val_loss: 0.2511\n",
      "Epoch 19/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2429 - val_loss: 0.2607\n",
      "Epoch 20/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2374 - val_loss: 0.2907\n",
      "Epoch 21/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2377 - val_loss: 0.2531\n",
      "Epoch 22/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2348 - val_loss: 0.2394\n",
      "Epoch 23/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.2304 - val_loss: 0.2690\n",
      "Epoch 24/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2304 - val_loss: 0.2359\n",
      "Epoch 25/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2271 - val_loss: 0.2454\n",
      "Epoch 26/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2253 - val_loss: 0.2471\n",
      "Epoch 27/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2210 - val_loss: 0.2380\n",
      "Epoch 28/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2182 - val_loss: 0.2430\n",
      "Epoch 29/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2169 - val_loss: 0.2317\n",
      "Epoch 30/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.2158 - val_loss: 0.2383\n",
      "Epoch 31/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2142 - val_loss: 0.2347\n",
      "Epoch 32/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2112 - val_loss: 0.2259\n",
      "Epoch 33/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2098 - val_loss: 0.2291\n",
      "Epoch 34/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2082 - val_loss: 0.2295\n",
      "Epoch 35/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2064 - val_loss: 0.2254\n",
      "Epoch 36/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2056 - val_loss: 0.2361\n",
      "Epoch 37/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2007 - val_loss: 0.2195\n",
      "Epoch 38/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2009 - val_loss: 0.2252\n",
      "Epoch 39/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2028 - val_loss: 0.2229\n",
      "Epoch 40/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1978 - val_loss: 0.2750\n",
      "Epoch 41/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1980 - val_loss: 0.2312\n",
      "Epoch 42/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1974 - val_loss: 0.2309\n",
      "Epoch 43/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1940 - val_loss: 0.2308\n",
      "Epoch 44/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1911 - val_loss: 0.2216\n",
      "Epoch 45/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1923 - val_loss: 0.2192\n",
      "Epoch 46/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1920 - val_loss: 0.2192\n",
      "Epoch 47/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1919 - val_loss: 0.2203\n",
      "Epoch 48/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1883 - val_loss: 0.2585\n",
      "Epoch 49/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1868 - val_loss: 0.2292\n",
      "Epoch 50/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1854 - val_loss: 0.2171\n",
      "Epoch 51/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1836 - val_loss: 0.2177\n",
      "Epoch 52/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1824 - val_loss: 0.2159\n",
      "Epoch 53/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1820 - val_loss: 0.2150\n",
      "Epoch 54/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1809 - val_loss: 0.2234\n",
      "Epoch 55/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1801 - val_loss: 0.2201\n",
      "Epoch 56/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1807 - val_loss: 0.2180\n",
      "Epoch 57/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1797 - val_loss: 0.2165\n",
      "Epoch 58/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1793 - val_loss: 0.2126\n",
      "Epoch 59/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1771 - val_loss: 0.2163\n",
      "Epoch 60/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1785 - val_loss: 0.2149\n",
      "Epoch 61/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1731 - val_loss: 0.2287\n",
      "Epoch 62/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1730 - val_loss: 0.2182\n",
      "Epoch 63/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1735 - val_loss: 0.2041\n",
      "Epoch 64/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1717 - val_loss: 0.2163\n",
      "Epoch 65/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1692 - val_loss: 0.2072\n",
      "Epoch 66/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1681 - val_loss: 0.2149\n",
      "Epoch 67/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1678 - val_loss: 0.2261\n",
      "Epoch 68/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1685 - val_loss: 0.2090\n",
      "Epoch 69/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1667 - val_loss: 0.2097\n",
      "Epoch 70/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1671 - val_loss: 0.2172\n",
      "Epoch 71/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1698 - val_loss: 0.2131\n",
      "Epoch 72/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1643 - val_loss: 0.2016\n",
      "Epoch 73/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1675 - val_loss: 0.2119\n",
      "Epoch 74/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1656 - val_loss: 0.2005\n",
      "Epoch 75/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1642 - val_loss: 0.2019\n",
      "Epoch 76/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1628 - val_loss: 0.2041\n",
      "Epoch 77/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1624 - val_loss: 0.2169\n",
      "Epoch 78/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1609 - val_loss: 0.2256\n",
      "Epoch 79/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1597 - val_loss: 0.2274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1600 - val_loss: 0.2146\n",
      "Epoch 81/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1583 - val_loss: 0.2043\n",
      "Epoch 82/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1606 - val_loss: 0.2042\n",
      "Epoch 83/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1581 - val_loss: 0.2149\n",
      "Epoch 84/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1575 - val_loss: 0.2071\n",
      "Epoch 85/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1566 - val_loss: 0.2085\n",
      "Epoch 86/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1558 - val_loss: 0.2148\n",
      "Epoch 87/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1553 - val_loss: 0.2051\n",
      "Epoch 88/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1559 - val_loss: 0.2075\n",
      "Epoch 89/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1545 - val_loss: 0.2072\n",
      "Epoch 90/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1523 - val_loss: 0.2079\n",
      "Epoch 91/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1564 - val_loss: 0.2058\n",
      "Epoch 92/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1522 - val_loss: 0.2043\n",
      "Epoch 93/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1544 - val_loss: 0.2043\n",
      "Epoch 94/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1498 - val_loss: 0.2063\n",
      "Epoch 95/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1518 - val_loss: 0.2120\n",
      "Epoch 96/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1523 - val_loss: 0.2045\n",
      "Epoch 97/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1482 - val_loss: 0.2023\n",
      "Epoch 98/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1509 - val_loss: 0.2394\n",
      "Epoch 99/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1489 - val_loss: 0.2045\n",
      "Epoch 100/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1475 - val_loss: 0.2077\n",
      "Epoch 101/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1483 - val_loss: 0.2014\n",
      "Epoch 102/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1486 - val_loss: 0.2059\n",
      "Epoch 103/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1485 - val_loss: 0.2170\n",
      "Epoch 104/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1471 - val_loss: 0.2061\n",
      "24\n",
      "Epoch 1/300\n",
      "1048/1048 [==============================] - 6s 5ms/step - loss: 2.5197 - val_loss: 0.9659\n",
      "Epoch 2/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.6037 - val_loss: 0.3900\n",
      "Epoch 3/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3896 - val_loss: 0.3370\n",
      "Epoch 4/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3547 - val_loss: 0.3197\n",
      "Epoch 5/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3338 - val_loss: 0.3008\n",
      "Epoch 6/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3224 - val_loss: 0.3014\n",
      "Epoch 7/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3099 - val_loss: 0.2925\n",
      "Epoch 8/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3025 - val_loss: 0.3119\n",
      "Epoch 9/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2919 - val_loss: 0.3683\n",
      "Epoch 10/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2853 - val_loss: 0.2748\n",
      "Epoch 11/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2793 - val_loss: 0.2692\n",
      "Epoch 12/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2736 - val_loss: 0.3002\n",
      "Epoch 13/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2685 - val_loss: 0.2609\n",
      "Epoch 14/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2671 - val_loss: 0.2676\n",
      "Epoch 15/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2614 - val_loss: 0.2802\n",
      "Epoch 16/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2553 - val_loss: 0.2629\n",
      "Epoch 17/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2496 - val_loss: 0.2614\n",
      "Epoch 18/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2501 - val_loss: 0.2548\n",
      "Epoch 19/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2440 - val_loss: 0.2720\n",
      "Epoch 20/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2429 - val_loss: 0.2487\n",
      "Epoch 21/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2390 - val_loss: 0.2423\n",
      "Epoch 22/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2374 - val_loss: 0.2437\n",
      "Epoch 23/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2329 - val_loss: 0.2492\n",
      "Epoch 24/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2308 - val_loss: 0.2385\n",
      "Epoch 25/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2301 - val_loss: 0.2417\n",
      "Epoch 26/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2265 - val_loss: 0.2420\n",
      "Epoch 27/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2235 - val_loss: 0.2859\n",
      "Epoch 28/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2217 - val_loss: 0.2347\n",
      "Epoch 29/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2183 - val_loss: 0.2447\n",
      "Epoch 30/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2178 - val_loss: 0.2438\n",
      "Epoch 31/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2159 - val_loss: 0.2379\n",
      "Epoch 32/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2120 - val_loss: 0.2360\n",
      "Epoch 33/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2090 - val_loss: 0.2382\n",
      "Epoch 34/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2100 - val_loss: 0.2515\n",
      "Epoch 35/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2071 - val_loss: 0.2226\n",
      "Epoch 36/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2057 - val_loss: 0.2269\n",
      "Epoch 37/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2036 - val_loss: 0.2280\n",
      "Epoch 38/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2022 - val_loss: 0.2278\n",
      "Epoch 39/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2017 - val_loss: 0.2315\n",
      "Epoch 40/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1967 - val_loss: 0.2299\n",
      "Epoch 41/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1993 - val_loss: 0.2230\n",
      "Epoch 42/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1969 - val_loss: 0.2285\n",
      "Epoch 43/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1970 - val_loss: 0.2481\n",
      "Epoch 44/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1950 - val_loss: 0.2210\n",
      "Epoch 45/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1952 - val_loss: 0.2366\n",
      "Epoch 46/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1943 - val_loss: 0.2355\n",
      "Epoch 47/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1899 - val_loss: 0.2520\n",
      "Epoch 48/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1899 - val_loss: 0.2290\n",
      "Epoch 49/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1888 - val_loss: 0.2254\n",
      "Epoch 50/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1854 - val_loss: 0.2194\n",
      "Epoch 51/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1859 - val_loss: 0.2179\n",
      "Epoch 52/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1847 - val_loss: 0.2207\n",
      "Epoch 53/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1816 - val_loss: 0.2326\n",
      "Epoch 54/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1821 - val_loss: 0.2277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1823 - val_loss: 0.2228\n",
      "Epoch 56/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1792 - val_loss: 0.2252\n",
      "Epoch 57/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1788 - val_loss: 0.2287\n",
      "Epoch 58/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1788 - val_loss: 0.2201\n",
      "Epoch 59/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1778 - val_loss: 0.2217\n",
      "Epoch 60/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1761 - val_loss: 0.2224\n",
      "Epoch 61/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1778 - val_loss: 0.2208\n",
      "Epoch 62/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1739 - val_loss: 0.2162\n",
      "Epoch 63/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1739 - val_loss: 0.2186\n",
      "Epoch 64/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1738 - val_loss: 0.2239\n",
      "Epoch 65/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1731 - val_loss: 0.2172\n",
      "Epoch 66/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1715 - val_loss: 0.2144\n",
      "Epoch 67/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1698 - val_loss: 0.2213\n",
      "Epoch 68/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1710 - val_loss: 0.2174\n",
      "Epoch 69/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1685 - val_loss: 0.2625\n",
      "Epoch 70/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1663 - val_loss: 0.2176\n",
      "Epoch 71/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1676 - val_loss: 0.2124\n",
      "Epoch 72/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1695 - val_loss: 0.2164\n",
      "Epoch 73/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1660 - val_loss: 0.2150\n",
      "Epoch 74/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1644 - val_loss: 0.2098\n",
      "Epoch 75/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1660 - val_loss: 0.2205\n",
      "Epoch 76/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1613 - val_loss: 0.2173\n",
      "Epoch 77/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1619 - val_loss: 0.2169\n",
      "Epoch 78/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1636 - val_loss: 0.2188\n",
      "Epoch 79/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1641 - val_loss: 0.2149\n",
      "Epoch 80/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1598 - val_loss: 0.2193\n",
      "Epoch 81/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1613 - val_loss: 0.2157\n",
      "Epoch 82/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1595 - val_loss: 0.2121\n",
      "Epoch 83/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1593 - val_loss: 0.2194\n",
      "Epoch 84/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1577 - val_loss: 0.2329\n",
      "Epoch 85/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1593 - val_loss: 0.2187\n",
      "Epoch 86/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1565 - val_loss: 0.2123\n",
      "Epoch 87/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1574 - val_loss: 0.2094\n",
      "Epoch 88/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1567 - val_loss: 0.2134\n",
      "Epoch 89/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1571 - val_loss: 0.2080\n",
      "Epoch 90/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1552 - val_loss: 0.2084\n",
      "Epoch 91/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1554 - val_loss: 0.2098\n",
      "Epoch 92/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1534 - val_loss: 0.2099\n",
      "Epoch 93/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1533 - val_loss: 0.2121\n",
      "Epoch 94/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1532 - val_loss: 0.2154\n",
      "Epoch 95/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1539 - val_loss: 0.2292\n",
      "Epoch 96/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1527 - val_loss: 0.2091\n",
      "Epoch 97/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1546 - val_loss: 0.2040\n",
      "Epoch 98/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1526 - val_loss: 0.2073\n",
      "Epoch 99/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1507 - val_loss: 0.2082\n",
      "Epoch 100/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1524 - val_loss: 0.2176\n",
      "Epoch 101/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1492 - val_loss: 0.2208\n",
      "Epoch 102/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1513 - val_loss: 0.2087\n",
      "Epoch 103/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1485 - val_loss: 0.2129\n",
      "Epoch 104/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1497 - val_loss: 0.2117\n",
      "Epoch 105/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1465 - val_loss: 0.2141\n",
      "Epoch 106/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1462 - val_loss: 0.2052\n",
      "Epoch 107/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1455 - val_loss: 0.2068\n",
      "Epoch 108/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1455 - val_loss: 0.1997\n",
      "Epoch 109/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1464 - val_loss: 0.2099\n",
      "Epoch 110/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1453 - val_loss: 0.2035\n",
      "Epoch 111/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1442 - val_loss: 0.2046\n",
      "Epoch 112/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1431 - val_loss: 0.2257\n",
      "Epoch 113/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1449 - val_loss: 0.2040\n",
      "Epoch 114/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1453 - val_loss: 0.2138\n",
      "Epoch 115/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1423 - val_loss: 0.2073\n",
      "Epoch 116/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1420 - val_loss: 0.2046\n",
      "Epoch 117/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1424 - val_loss: 0.2154\n",
      "Epoch 118/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1449 - val_loss: 0.2072\n",
      "Epoch 119/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1425 - val_loss: 0.2043\n",
      "Epoch 120/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1404 - val_loss: 0.2092\n",
      "Epoch 121/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1414 - val_loss: 0.2163\n",
      "Epoch 122/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1413 - val_loss: 0.2043\n",
      "Epoch 123/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1427 - val_loss: 0.2069\n",
      "Epoch 124/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1392 - val_loss: 0.2001\n",
      "Epoch 125/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1381 - val_loss: 0.2038\n",
      "Epoch 126/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1390 - val_loss: 0.2038\n",
      "Epoch 127/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1374 - val_loss: 0.2090\n",
      "Epoch 128/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1392 - val_loss: 0.2035\n",
      "Epoch 129/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1373 - val_loss: 0.1998\n",
      "Epoch 130/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1392 - val_loss: 0.2091\n",
      "Epoch 131/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1350 - val_loss: 0.1980\n",
      "Epoch 132/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1392 - val_loss: 0.2029\n",
      "Epoch 133/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1365 - val_loss: 0.2126\n",
      "Epoch 134/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1374 - val_loss: 0.2066\n",
      "Epoch 135/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1387 - val_loss: 0.2088\n",
      "Epoch 136/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1346 - val_loss: 0.2044\n",
      "Epoch 137/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1349 - val_loss: 0.2031\n",
      "Epoch 138/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1345 - val_loss: 0.2219\n",
      "Epoch 139/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1350 - val_loss: 0.1996\n",
      "Epoch 140/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1337 - val_loss: 0.1997\n",
      "Epoch 141/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1335 - val_loss: 0.2050\n",
      "Epoch 142/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1322 - val_loss: 0.2052\n",
      "Epoch 143/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1331 - val_loss: 0.2225\n",
      "Epoch 144/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1348 - val_loss: 0.2083\n",
      "Epoch 145/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1339 - val_loss: 0.2120\n",
      "Epoch 146/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1323 - val_loss: 0.1982\n",
      "Epoch 147/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1291 - val_loss: 0.2071\n",
      "Epoch 148/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1325 - val_loss: 0.2062\n",
      "Epoch 149/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1348 - val_loss: 0.2337\n",
      "Epoch 150/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1320 - val_loss: 0.2300\n",
      "Epoch 151/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1295 - val_loss: 0.2052\n",
      "Epoch 152/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1291 - val_loss: 0.1995\n",
      "Epoch 153/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1311 - val_loss: 0.2091\n",
      "Epoch 154/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1277 - val_loss: 0.2043\n",
      "Epoch 155/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1308 - val_loss: 0.2077\n",
      "Epoch 156/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1300 - val_loss: 0.2031\n",
      "Epoch 157/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1294 - val_loss: 0.2111\n",
      "Epoch 158/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1285 - val_loss: 0.2059\n",
      "Epoch 159/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1287 - val_loss: 0.2068\n",
      "Epoch 160/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1287 - val_loss: 0.2040\n",
      "Epoch 161/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1282 - val_loss: 0.2059\n",
      "3\n",
      "Epoch 1/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 2.4972 - val_loss: 0.8937\n",
      "Epoch 2/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.6029 - val_loss: 0.3672\n",
      "Epoch 3/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3866 - val_loss: 0.3280\n",
      "Epoch 4/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3531 - val_loss: 0.3378\n",
      "Epoch 5/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3314 - val_loss: 0.3223\n",
      "Epoch 6/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3220 - val_loss: 0.3087\n",
      "Epoch 7/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.3086 - val_loss: 0.2944\n",
      "Epoch 8/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2993 - val_loss: 0.2901\n",
      "Epoch 9/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2930 - val_loss: 0.2724\n",
      "Epoch 10/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2863 - val_loss: 0.2829\n",
      "Epoch 11/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2790 - val_loss: 0.2707\n",
      "Epoch 12/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2740 - val_loss: 0.2799\n",
      "Epoch 13/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2713 - val_loss: 0.2609\n",
      "Epoch 14/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2651 - val_loss: 0.2607\n",
      "Epoch 15/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2604 - val_loss: 0.2687\n",
      "Epoch 16/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2590 - val_loss: 0.2524\n",
      "Epoch 17/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2536 - val_loss: 0.2759\n",
      "Epoch 18/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2483 - val_loss: 0.2569\n",
      "Epoch 19/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2448 - val_loss: 0.2613\n",
      "Epoch 20/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2425 - val_loss: 0.2464\n",
      "Epoch 21/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2392 - val_loss: 0.3247\n",
      "Epoch 22/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2388 - val_loss: 0.2466\n",
      "Epoch 23/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2350 - val_loss: 0.2473\n",
      "Epoch 24/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2305 - val_loss: 0.2849\n",
      "Epoch 25/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2260 - val_loss: 0.2487\n",
      "Epoch 26/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2273 - val_loss: 0.2347\n",
      "Epoch 27/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2208 - val_loss: 0.2572\n",
      "Epoch 28/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2211 - val_loss: 0.2346\n",
      "Epoch 29/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2197 - val_loss: 0.2568\n",
      "Epoch 30/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2172 - val_loss: 0.2338\n",
      "Epoch 31/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2144 - val_loss: 0.2348\n",
      "Epoch 32/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2137 - val_loss: 0.2349\n",
      "Epoch 33/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2113 - val_loss: 0.2528\n",
      "Epoch 34/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2078 - val_loss: 0.2436\n",
      "Epoch 35/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2104 - val_loss: 0.2304\n",
      "Epoch 36/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2046 - val_loss: 0.2538\n",
      "Epoch 37/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2038 - val_loss: 0.2288\n",
      "Epoch 38/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2048 - val_loss: 0.2246\n",
      "Epoch 39/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2015 - val_loss: 0.2309\n",
      "Epoch 40/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1998 - val_loss: 0.2318\n",
      "Epoch 41/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1992 - val_loss: 0.2428\n",
      "Epoch 42/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1967 - val_loss: 0.2423\n",
      "Epoch 43/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1950 - val_loss: 0.2526\n",
      "Epoch 44/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1943 - val_loss: 0.2364\n",
      "Epoch 45/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1923 - val_loss: 0.2356\n",
      "Epoch 46/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1919 - val_loss: 0.2534\n",
      "Epoch 47/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1897 - val_loss: 0.2268\n",
      "Epoch 48/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1890 - val_loss: 0.2266\n",
      "Epoch 49/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1872 - val_loss: 0.2257\n",
      "Epoch 50/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1868 - val_loss: 0.2188\n",
      "Epoch 51/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1856 - val_loss: 0.2191\n",
      "Epoch 52/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1864 - val_loss: 0.2245\n",
      "Epoch 53/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1817 - val_loss: 0.2750\n",
      "Epoch 54/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1828 - val_loss: 0.2223\n",
      "Epoch 55/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1830 - val_loss: 0.2199\n",
      "Epoch 56/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1810 - val_loss: 0.2206\n",
      "Epoch 57/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1804 - val_loss: 0.2202\n",
      "Epoch 58/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1781 - val_loss: 0.2164\n",
      "Epoch 59/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1790 - val_loss: 0.2163\n",
      "Epoch 60/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1774 - val_loss: 0.2224\n",
      "Epoch 61/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1750 - val_loss: 0.2203\n",
      "Epoch 62/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1739 - val_loss: 0.2171\n",
      "Epoch 63/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1739 - val_loss: 0.2316\n",
      "Epoch 64/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1701 - val_loss: 0.2392\n",
      "Epoch 65/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1738 - val_loss: 0.2224\n",
      "Epoch 66/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1719 - val_loss: 0.2122\n",
      "Epoch 67/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1710 - val_loss: 0.2147\n",
      "Epoch 68/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1703 - val_loss: 0.2190\n",
      "Epoch 69/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1710 - val_loss: 0.2113\n",
      "Epoch 70/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1695 - val_loss: 0.2118\n",
      "Epoch 71/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1685 - val_loss: 0.2226\n",
      "Epoch 72/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1691 - val_loss: 0.2210\n",
      "Epoch 73/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1650 - val_loss: 0.2173\n",
      "Epoch 74/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1646 - val_loss: 0.2167\n",
      "Epoch 75/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1678 - val_loss: 0.2333\n",
      "Epoch 76/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1630 - val_loss: 0.2256\n",
      "Epoch 77/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1627 - val_loss: 0.2134\n",
      "Epoch 78/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1647 - val_loss: 0.2147\n",
      "Epoch 79/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1638 - val_loss: 0.2148\n",
      "Epoch 80/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1622 - val_loss: 0.2432\n",
      "Epoch 81/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1604 - val_loss: 0.2110\n",
      "Epoch 82/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1597 - val_loss: 0.2094\n",
      "Epoch 83/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1608 - val_loss: 0.2156\n",
      "Epoch 84/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1584 - val_loss: 0.2201\n",
      "Epoch 85/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1585 - val_loss: 0.2115\n",
      "Epoch 86/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1586 - val_loss: 0.2095\n",
      "Epoch 87/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1565 - val_loss: 0.2116\n",
      "Epoch 88/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1553 - val_loss: 0.2138\n",
      "Epoch 89/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1548 - val_loss: 0.2110\n",
      "Epoch 90/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1537 - val_loss: 0.2166\n",
      "Epoch 91/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1549 - val_loss: 0.2342\n",
      "Epoch 92/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1542 - val_loss: 0.2103\n",
      "Epoch 93/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1542 - val_loss: 0.2183\n",
      "Epoch 94/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1533 - val_loss: 0.2129\n",
      "Epoch 95/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1542 - val_loss: 0.2079\n",
      "Epoch 96/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1511 - val_loss: 0.2092\n",
      "Epoch 97/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1513 - val_loss: 0.2169\n",
      "Epoch 98/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1512 - val_loss: 0.2110\n",
      "Epoch 99/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1489 - val_loss: 0.2215\n",
      "Epoch 100/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1509 - val_loss: 0.2136\n",
      "Epoch 101/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1512 - val_loss: 0.2105\n",
      "Epoch 102/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1475 - val_loss: 0.2164\n",
      "Epoch 103/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1493 - val_loss: 0.2092\n",
      "Epoch 104/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1482 - val_loss: 0.2125\n",
      "Epoch 105/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1472 - val_loss: 0.2136\n",
      "Epoch 106/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1485 - val_loss: 0.2058\n",
      "Epoch 107/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1455 - val_loss: 0.2054\n",
      "Epoch 108/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1477 - val_loss: 0.2148\n",
      "Epoch 109/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1471 - val_loss: 0.2067\n",
      "Epoch 110/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1489 - val_loss: 0.2229\n",
      "Epoch 111/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1457 - val_loss: 0.2165\n",
      "Epoch 112/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1423 - val_loss: 0.2103\n",
      "Epoch 113/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1443 - val_loss: 0.2108\n",
      "Epoch 114/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1435 - val_loss: 0.2106\n",
      "Epoch 115/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1458 - val_loss: 0.2116\n",
      "Epoch 116/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1420 - val_loss: 0.2098\n",
      "Epoch 117/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1430 - val_loss: 0.2060\n",
      "Epoch 118/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1412 - val_loss: 0.2066\n",
      "Epoch 119/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1436 - val_loss: 0.2218\n",
      "Epoch 120/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1391 - val_loss: 0.2143\n",
      "Epoch 121/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1411 - val_loss: 0.2107\n",
      "Epoch 122/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1409 - val_loss: 0.2156\n",
      "Epoch 123/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1397 - val_loss: 0.2184\n",
      "Epoch 124/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1398 - val_loss: 0.2083\n",
      "Epoch 125/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1394 - val_loss: 0.2084\n",
      "Epoch 126/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1394 - val_loss: 0.2085\n",
      "Epoch 127/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1411 - val_loss: 0.2042\n",
      "Epoch 128/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1379 - val_loss: 0.2074\n",
      "Epoch 129/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1372 - val_loss: 0.2137\n",
      "Epoch 130/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1368 - val_loss: 0.2177\n",
      "Epoch 131/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1395 - val_loss: 0.2152\n",
      "Epoch 132/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1384 - val_loss: 0.2132\n",
      "Epoch 133/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1390 - val_loss: 0.2036\n",
      "Epoch 134/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1390 - val_loss: 0.2071\n",
      "Epoch 135/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1341 - val_loss: 0.2076\n",
      "Epoch 136/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1366 - val_loss: 0.2143\n",
      "Epoch 137/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1354 - val_loss: 0.2089\n",
      "Epoch 138/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1365 - val_loss: 0.2091\n",
      "Epoch 139/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1328 - val_loss: 0.2044\n",
      "Epoch 140/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1339 - val_loss: 0.2171\n",
      "Epoch 141/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1337 - val_loss: 0.2082\n",
      "Epoch 142/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1362 - val_loss: 0.2150\n",
      "Epoch 143/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1314 - val_loss: 0.2062\n",
      "Epoch 144/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1318 - val_loss: 0.2103\n",
      "Epoch 145/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1331 - val_loss: 0.2063\n",
      "Epoch 146/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1335 - val_loss: 0.2057\n",
      "Epoch 147/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1337 - val_loss: 0.2194\n",
      "Epoch 148/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1319 - val_loss: 0.2075\n",
      "Epoch 149/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1331 - val_loss: 0.2121\n",
      "Epoch 150/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1287 - val_loss: 0.2079\n",
      "Epoch 151/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1329 - val_loss: 0.2011\n",
      "Epoch 152/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1319 - val_loss: 0.2064\n",
      "Epoch 153/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1297 - val_loss: 0.2081\n",
      "Epoch 154/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1297 - val_loss: 0.2015\n",
      "Epoch 155/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1297 - val_loss: 0.2048\n",
      "Epoch 156/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1322 - val_loss: 0.2071\n",
      "Epoch 157/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1303 - val_loss: 0.2075\n",
      "Epoch 158/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1308 - val_loss: 0.2077\n",
      "Epoch 159/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1285 - val_loss: 0.2039\n",
      "Epoch 160/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1297 - val_loss: 0.2153\n",
      "Epoch 161/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1302 - val_loss: 0.2025\n",
      "Epoch 162/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1294 - val_loss: 0.2138\n",
      "Epoch 163/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1280 - val_loss: 0.2298\n",
      "Epoch 164/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1292 - val_loss: 0.2034\n",
      "Epoch 165/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1292 - val_loss: 0.2000\n",
      "Epoch 166/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1293 - val_loss: 0.2021\n",
      "Epoch 167/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1280 - val_loss: 0.2030\n",
      "Epoch 168/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1283 - val_loss: 0.2105\n",
      "Epoch 169/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1265 - val_loss: 0.2159\n",
      "Epoch 170/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1280 - val_loss: 0.2066\n",
      "Epoch 171/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1284 - val_loss: 0.2040\n",
      "Epoch 172/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1233 - val_loss: 0.2021\n",
      "Epoch 173/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1269 - val_loss: 0.2084\n",
      "Epoch 174/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1232 - val_loss: 0.2019\n",
      "Epoch 175/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1235 - val_loss: 0.2036\n",
      "Epoch 176/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1252 - val_loss: 0.2078\n",
      "Epoch 177/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1263 - val_loss: 0.2075\n",
      "Epoch 178/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1255 - val_loss: 0.2010\n",
      "Epoch 179/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1258 - val_loss: 0.2041\n",
      "Epoch 180/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1247 - val_loss: 0.2041\n",
      "Epoch 181/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1249 - val_loss: 0.2049\n",
      "Epoch 182/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1249 - val_loss: 0.2097\n",
      "Epoch 183/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1230 - val_loss: 0.2114\n",
      "Epoch 184/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1240 - val_loss: 0.2056\n",
      "Epoch 185/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1261 - val_loss: 0.2113\n",
      "Epoch 186/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1235 - val_loss: 0.2019\n",
      "Epoch 187/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1231 - val_loss: 0.2051\n",
      "Epoch 188/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1206 - val_loss: 0.2091\n",
      "Epoch 189/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1225 - val_loss: 0.2162\n",
      "Epoch 190/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1229 - val_loss: 0.2022\n",
      "Epoch 191/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1215 - val_loss: 0.2106\n",
      "Epoch 192/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1218 - val_loss: 0.2239\n",
      "Epoch 193/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1222 - val_loss: 0.2067\n",
      "Epoch 194/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1209 - val_loss: 0.2062\n",
      "Epoch 195/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1223 - val_loss: 0.2054\n",
      "123\n",
      "Epoch 1/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 2.6074 - val_loss: 0.9022\n",
      "Epoch 2/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.6266 - val_loss: 0.3644\n",
      "Epoch 3/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3808 - val_loss: 0.3333\n",
      "Epoch 4/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3514 - val_loss: 0.3318\n",
      "Epoch 5/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3329 - val_loss: 0.3303\n",
      "Epoch 6/300\n",
      "1048/1048 [==============================] - 4s 3ms/step - loss: 0.3209 - val_loss: 0.3018\n",
      "Epoch 7/300\n",
      "1048/1048 [==============================] - 4s 3ms/step - loss: 0.3101 - val_loss: 0.3163\n",
      "Epoch 8/300\n",
      "1048/1048 [==============================] - 4s 3ms/step - loss: 0.2989 - val_loss: 0.2903\n",
      "Epoch 9/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2908 - val_loss: 0.2986\n",
      "Epoch 10/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2826 - val_loss: 0.3246\n",
      "Epoch 11/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2792 - val_loss: 0.2873\n",
      "Epoch 12/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2751 - val_loss: 0.2685\n",
      "Epoch 13/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2700 - val_loss: 0.2740\n",
      "Epoch 14/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2641 - val_loss: 0.2635\n",
      "Epoch 15/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2569 - val_loss: 0.2479\n",
      "Epoch 16/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2537 - val_loss: 0.2605\n",
      "Epoch 17/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2523 - val_loss: 0.2503\n",
      "Epoch 18/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2493 - val_loss: 0.2669\n",
      "Epoch 19/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2437 - val_loss: 0.2442\n",
      "Epoch 20/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2425 - val_loss: 0.3040\n",
      "Epoch 21/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2378 - val_loss: 0.2347\n",
      "Epoch 22/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2352 - val_loss: 0.2578\n",
      "Epoch 23/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2328 - val_loss: 0.2425\n",
      "Epoch 24/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2316 - val_loss: 0.2405\n",
      "Epoch 25/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2295 - val_loss: 0.2460\n",
      "Epoch 26/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2272 - val_loss: 0.2582\n",
      "Epoch 27/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2237 - val_loss: 0.2374\n",
      "Epoch 28/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2188 - val_loss: 0.2351\n",
      "Epoch 29/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2183 - val_loss: 0.2361\n",
      "Epoch 30/300\n",
      "1048/1048 [==============================] - 4s 3ms/step - loss: 0.2151 - val_loss: 0.2317\n",
      "Epoch 31/300\n",
      "1048/1048 [==============================] - 4s 3ms/step - loss: 0.2149 - val_loss: 0.2287\n",
      "Epoch 32/300\n",
      "1048/1048 [==============================] - 4s 3ms/step - loss: 0.2138 - val_loss: 0.2267\n",
      "Epoch 33/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2088 - val_loss: 0.2478\n",
      "Epoch 34/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2099 - val_loss: 0.2262\n",
      "Epoch 35/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2080 - val_loss: 0.2254\n",
      "Epoch 36/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2057 - val_loss: 0.2226\n",
      "Epoch 37/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2010 - val_loss: 0.2316\n",
      "Epoch 38/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2039 - val_loss: 0.2290\n",
      "Epoch 39/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2015 - val_loss: 0.2413\n",
      "Epoch 40/300\n",
      "1048/1048 [==============================] - 4s 3ms/step - loss: 0.1998 - val_loss: 0.2200\n",
      "Epoch 41/300\n",
      "1048/1048 [==============================] - 4s 3ms/step - loss: 0.1983 - val_loss: 0.2162\n",
      "Epoch 42/300\n",
      "1048/1048 [==============================] - 4s 3ms/step - loss: 0.1984 - val_loss: 0.2382\n",
      "Epoch 43/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1942 - val_loss: 0.2946\n",
      "Epoch 44/300\n",
      "1048/1048 [==============================] - 4s 3ms/step - loss: 0.1935 - val_loss: 0.2324\n",
      "Epoch 45/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1914 - val_loss: 0.2160\n",
      "Epoch 46/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1906 - val_loss: 0.2168\n",
      "Epoch 47/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1901 - val_loss: 0.2453\n",
      "Epoch 48/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1912 - val_loss: 0.2165\n",
      "Epoch 49/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1861 - val_loss: 0.2310\n",
      "Epoch 50/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1854 - val_loss: 0.2291\n",
      "Epoch 51/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1846 - val_loss: 0.2172\n",
      "Epoch 52/300\n",
      "1048/1048 [==============================] - 4s 3ms/step - loss: 0.1847 - val_loss: 0.2500\n",
      "Epoch 53/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1794 - val_loss: 0.2141\n",
      "Epoch 54/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1800 - val_loss: 0.2158\n",
      "Epoch 55/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1803 - val_loss: 0.2165\n",
      "Epoch 56/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1796 - val_loss: 0.2587\n",
      "Epoch 57/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1789 - val_loss: 0.2140\n",
      "Epoch 58/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1819 - val_loss: 0.2138\n",
      "Epoch 59/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1780 - val_loss: 0.2114\n",
      "Epoch 60/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1757 - val_loss: 0.2127\n",
      "Epoch 61/300\n",
      "1048/1048 [==============================] - 4s 3ms/step - loss: 0.1778 - val_loss: 0.2759\n",
      "Epoch 62/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1784 - val_loss: 0.2127\n",
      "Epoch 63/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1721 - val_loss: 0.2164\n",
      "Epoch 64/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1709 - val_loss: 0.2128\n",
      "Epoch 65/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1735 - val_loss: 0.2112\n",
      "Epoch 66/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1701 - val_loss: 0.2061\n",
      "Epoch 67/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1717 - val_loss: 0.2036\n",
      "Epoch 68/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1677 - val_loss: 0.2114\n",
      "Epoch 69/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1697 - val_loss: 0.2108\n",
      "Epoch 70/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1681 - val_loss: 0.2072\n",
      "Epoch 71/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1676 - val_loss: 0.2182\n",
      "Epoch 72/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1684 - val_loss: 0.2069\n",
      "Epoch 73/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1680 - val_loss: 0.2073\n",
      "Epoch 74/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1655 - val_loss: 0.2085\n",
      "Epoch 75/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1632 - val_loss: 0.2105\n",
      "Epoch 76/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1626 - val_loss: 0.2036\n",
      "Epoch 77/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1620 - val_loss: 0.2051\n",
      "Epoch 78/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1600 - val_loss: 0.2124\n",
      "Epoch 79/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1636 - val_loss: 0.2053\n",
      "Epoch 80/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1601 - val_loss: 0.2230\n",
      "Epoch 81/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1593 - val_loss: 0.2214\n",
      "Epoch 82/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1574 - val_loss: 0.2064\n",
      "Epoch 83/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1614 - val_loss: 0.2062\n",
      "Epoch 84/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1593 - val_loss: 0.2076\n",
      "Epoch 85/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1602 - val_loss: 0.2101\n",
      "Epoch 86/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1590 - val_loss: 0.2044\n",
      "Epoch 87/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1560 - val_loss: 0.2178\n",
      "Epoch 88/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1563 - val_loss: 0.2562\n",
      "Epoch 89/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1574 - val_loss: 0.1995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1543 - val_loss: 0.2247\n",
      "Epoch 91/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1548 - val_loss: 0.2060\n",
      "Epoch 92/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1543 - val_loss: 0.2061\n",
      "Epoch 93/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1538 - val_loss: 0.2060\n",
      "Epoch 94/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1539 - val_loss: 0.1992\n",
      "Epoch 95/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1516 - val_loss: 0.2162\n",
      "Epoch 96/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1535 - val_loss: 0.2082\n",
      "Epoch 97/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1504 - val_loss: 0.2035\n",
      "Epoch 98/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1514 - val_loss: 0.2136\n",
      "Epoch 99/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1501 - val_loss: 0.2049\n",
      "Epoch 100/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1498 - val_loss: 0.2047\n",
      "Epoch 101/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1487 - val_loss: 0.2019\n",
      "Epoch 102/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1486 - val_loss: 0.2064\n",
      "Epoch 103/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1477 - val_loss: 0.2017\n",
      "Epoch 104/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1472 - val_loss: 0.2067\n",
      "Epoch 105/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1459 - val_loss: 0.2120\n",
      "Epoch 106/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1455 - val_loss: 0.2009\n",
      "Epoch 107/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1470 - val_loss: 0.2045\n",
      "Epoch 108/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1452 - val_loss: 0.2047\n",
      "Epoch 109/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1457 - val_loss: 0.2040\n",
      "Epoch 110/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1455 - val_loss: 0.2177\n",
      "Epoch 111/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1467 - val_loss: 0.2028\n",
      "Epoch 112/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1415 - val_loss: 0.2009\n",
      "Epoch 113/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1451 - val_loss: 0.2127\n",
      "Epoch 114/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1428 - val_loss: 0.2007\n",
      "Epoch 115/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1439 - val_loss: 0.1980\n",
      "Epoch 116/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1451 - val_loss: 0.2002\n",
      "Epoch 117/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1448 - val_loss: 0.2032\n",
      "Epoch 118/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1399 - val_loss: 0.2008\n",
      "Epoch 119/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1435 - val_loss: 0.1972\n",
      "Epoch 120/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1415 - val_loss: 0.1988\n",
      "Epoch 121/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1388 - val_loss: 0.1976\n",
      "Epoch 122/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1425 - val_loss: 0.2011\n",
      "Epoch 123/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1397 - val_loss: 0.2115\n",
      "Epoch 124/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1386 - val_loss: 0.1985\n",
      "Epoch 125/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1380 - val_loss: 0.1974\n",
      "Epoch 126/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1363 - val_loss: 0.2118\n",
      "Epoch 127/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1394 - val_loss: 0.2000\n",
      "Epoch 128/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1394 - val_loss: 0.2016\n",
      "Epoch 129/300\n",
      "1048/1048 [==============================] - 4s 3ms/step - loss: 0.1401 - val_loss: 0.1988\n",
      "Epoch 130/300\n",
      "1048/1048 [==============================] - 4s 3ms/step - loss: 0.1383 - val_loss: 0.1999\n",
      "Epoch 131/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1370 - val_loss: 0.1991\n",
      "Epoch 132/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1373 - val_loss: 0.2029\n",
      "Epoch 133/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1360 - val_loss: 0.2011\n",
      "Epoch 134/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1356 - val_loss: 0.1989\n",
      "Epoch 135/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1354 - val_loss: 0.2025\n",
      "Epoch 136/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1348 - val_loss: 0.2036\n",
      "Epoch 137/300\n",
      "1048/1048 [==============================] - 4s 3ms/step - loss: 0.1367 - val_loss: 0.1993\n",
      "Epoch 138/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1370 - val_loss: 0.1977\n",
      "Epoch 139/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1339 - val_loss: 0.2048\n",
      "Epoch 140/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1360 - val_loss: 0.1995\n",
      "Epoch 141/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1341 - val_loss: 0.1989\n",
      "Epoch 142/300\n",
      "1048/1048 [==============================] - 4s 3ms/step - loss: 0.1355 - val_loss: 0.2091\n",
      "Epoch 143/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1349 - val_loss: 0.2164\n",
      "Epoch 144/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1316 - val_loss: 0.2037\n",
      "Epoch 145/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1322 - val_loss: 0.2086\n",
      "Epoch 146/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1325 - val_loss: 0.2004\n",
      "Epoch 147/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1308 - val_loss: 0.2016\n",
      "Epoch 148/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1302 - val_loss: 0.1978\n",
      "Epoch 149/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1319 - val_loss: 0.2014\n",
      "7\n",
      "Epoch 1/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 2.4798 - val_loss: 0.9793\n",
      "Epoch 2/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.5948 - val_loss: 0.3853\n",
      "Epoch 3/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3913 - val_loss: 0.3627\n",
      "Epoch 4/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3601 - val_loss: 0.3195\n",
      "Epoch 5/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3372 - val_loss: 0.3011\n",
      "Epoch 6/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3240 - val_loss: 0.2951\n",
      "Epoch 7/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3122 - val_loss: 0.3498\n",
      "Epoch 8/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3025 - val_loss: 0.2894\n",
      "Epoch 9/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2937 - val_loss: 0.2847\n",
      "Epoch 10/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2871 - val_loss: 0.2740\n",
      "Epoch 11/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2816 - val_loss: 0.2732\n",
      "Epoch 12/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2722 - val_loss: 0.2726\n",
      "Epoch 13/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2693 - val_loss: 0.2561\n",
      "Epoch 14/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2629 - val_loss: 0.3269\n",
      "Epoch 15/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2588 - val_loss: 0.2699\n",
      "Epoch 16/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2567 - val_loss: 0.2791\n",
      "Epoch 17/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2535 - val_loss: 0.2615\n",
      "Epoch 18/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2477 - val_loss: 0.2569\n",
      "Epoch 19/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2458 - val_loss: 0.2707\n",
      "Epoch 20/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2428 - val_loss: 0.2422\n",
      "Epoch 21/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2405 - val_loss: 0.2401\n",
      "Epoch 22/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2367 - val_loss: 0.2519\n",
      "Epoch 23/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2336 - val_loss: 0.2506\n",
      "Epoch 24/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2348 - val_loss: 0.2552\n",
      "Epoch 25/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2287 - val_loss: 0.2606\n",
      "Epoch 26/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2317 - val_loss: 0.2364\n",
      "Epoch 27/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2253 - val_loss: 0.2337\n",
      "Epoch 28/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2223 - val_loss: 0.2386\n",
      "Epoch 29/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2172 - val_loss: 0.2303\n",
      "Epoch 30/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2160 - val_loss: 0.2339\n",
      "Epoch 31/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2158 - val_loss: 0.2418\n",
      "Epoch 32/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2171 - val_loss: 0.2298\n",
      "Epoch 33/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2127 - val_loss: 0.2559\n",
      "Epoch 34/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2085 - val_loss: 0.2417\n",
      "Epoch 35/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2101 - val_loss: 0.2731\n",
      "Epoch 36/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2068 - val_loss: 0.2339\n",
      "Epoch 37/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2037 - val_loss: 0.2301\n",
      "Epoch 38/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2005 - val_loss: 0.2260\n",
      "Epoch 39/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2034 - val_loss: 0.2301\n",
      "Epoch 40/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1995 - val_loss: 0.2293\n",
      "Epoch 41/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1998 - val_loss: 0.2249\n",
      "Epoch 42/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1980 - val_loss: 0.2239\n",
      "Epoch 43/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1971 - val_loss: 0.2239\n",
      "Epoch 44/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1950 - val_loss: 0.2244\n",
      "Epoch 45/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1924 - val_loss: 0.2373\n",
      "Epoch 46/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1927 - val_loss: 0.2215\n",
      "Epoch 47/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1903 - val_loss: 0.2203\n",
      "Epoch 48/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1902 - val_loss: 0.2380\n",
      "Epoch 49/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1878 - val_loss: 0.2545\n",
      "Epoch 50/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1853 - val_loss: 0.2231\n",
      "Epoch 51/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1848 - val_loss: 0.2483\n",
      "Epoch 52/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1870 - val_loss: 0.2121\n",
      "Epoch 53/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1829 - val_loss: 0.2267\n",
      "Epoch 54/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1816 - val_loss: 0.2267\n",
      "Epoch 55/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1854 - val_loss: 0.2260\n",
      "Epoch 56/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1831 - val_loss: 0.2162\n",
      "Epoch 57/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1796 - val_loss: 0.2329\n",
      "Epoch 58/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1777 - val_loss: 0.2188\n",
      "Epoch 59/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1766 - val_loss: 0.2160\n",
      "Epoch 60/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1773 - val_loss: 0.2441\n",
      "Epoch 61/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1765 - val_loss: 0.2160\n",
      "Epoch 62/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1756 - val_loss: 0.2151\n",
      "Epoch 63/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1737 - val_loss: 0.2490\n",
      "Epoch 64/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1745 - val_loss: 0.2328\n",
      "Epoch 65/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1718 - val_loss: 0.2141\n",
      "Epoch 66/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1699 - val_loss: 0.2196\n",
      "Epoch 67/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1702 - val_loss: 0.2195\n",
      "Epoch 68/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1698 - val_loss: 0.2214\n",
      "Epoch 69/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1680 - val_loss: 0.2157\n",
      "Epoch 70/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1673 - val_loss: 0.2105\n",
      "Epoch 71/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1663 - val_loss: 0.2125\n",
      "Epoch 72/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1657 - val_loss: 0.2125\n",
      "Epoch 73/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1654 - val_loss: 0.2221\n",
      "Epoch 74/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1633 - val_loss: 0.2382\n",
      "Epoch 75/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1666 - val_loss: 0.2205\n",
      "Epoch 76/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1632 - val_loss: 0.2101\n",
      "Epoch 77/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1620 - val_loss: 0.2137\n",
      "Epoch 78/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1621 - val_loss: 0.2149\n",
      "Epoch 79/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1608 - val_loss: 0.2123\n",
      "Epoch 80/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1610 - val_loss: 0.2073\n",
      "Epoch 81/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1616 - val_loss: 0.2134\n",
      "Epoch 82/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1585 - val_loss: 0.2151\n",
      "Epoch 83/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1607 - val_loss: 0.2228\n",
      "Epoch 84/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1584 - val_loss: 0.2127\n",
      "Epoch 85/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1567 - val_loss: 0.2140\n",
      "Epoch 86/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1566 - val_loss: 0.2142\n",
      "Epoch 87/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1575 - val_loss: 0.2187\n",
      "Epoch 88/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1549 - val_loss: 0.2136\n",
      "Epoch 89/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1572 - val_loss: 0.2151\n",
      "Epoch 90/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1524 - val_loss: 0.2091\n",
      "Epoch 91/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1540 - val_loss: 0.2139\n",
      "Epoch 92/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1544 - val_loss: 0.2098\n",
      "Epoch 93/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1533 - val_loss: 0.2168\n",
      "Epoch 94/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1536 - val_loss: 0.2124\n",
      "Epoch 95/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1512 - val_loss: 0.2072\n",
      "Epoch 96/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1521 - val_loss: 0.2094\n",
      "Epoch 97/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1507 - val_loss: 0.2039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1506 - val_loss: 0.2207\n",
      "Epoch 99/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1505 - val_loss: 0.2073\n",
      "Epoch 100/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1475 - val_loss: 0.2113\n",
      "Epoch 101/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1515 - val_loss: 0.2049\n",
      "Epoch 102/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1483 - val_loss: 0.2156\n",
      "Epoch 103/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1476 - val_loss: 0.2114\n",
      "Epoch 104/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1432 - val_loss: 0.2108\n",
      "Epoch 105/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1468 - val_loss: 0.2315\n",
      "Epoch 106/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1467 - val_loss: 0.2210\n",
      "Epoch 107/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1441 - val_loss: 0.2102\n",
      "Epoch 108/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1447 - val_loss: 0.2149\n",
      "Epoch 109/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1430 - val_loss: 0.2079\n",
      "Epoch 110/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1428 - val_loss: 0.2065\n",
      "Epoch 111/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1448 - val_loss: 0.2000\n",
      "Epoch 112/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1459 - val_loss: 0.2048\n",
      "Epoch 113/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1442 - val_loss: 0.2176\n",
      "Epoch 114/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1418 - val_loss: 0.1994\n",
      "Epoch 115/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1416 - val_loss: 0.2121\n",
      "Epoch 116/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1426 - val_loss: 0.2402\n",
      "Epoch 117/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1430 - val_loss: 0.2012\n",
      "Epoch 118/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1436 - val_loss: 0.2043\n",
      "Epoch 119/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1424 - val_loss: 0.2092\n",
      "Epoch 120/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1420 - val_loss: 0.2189\n",
      "Epoch 121/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1391 - val_loss: 0.2078\n",
      "Epoch 122/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1380 - val_loss: 0.2057\n",
      "Epoch 123/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1392 - val_loss: 0.2058\n",
      "Epoch 124/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1394 - val_loss: 0.2023\n",
      "Epoch 125/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1372 - val_loss: 0.2039\n",
      "Epoch 126/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1344 - val_loss: 0.2119\n",
      "Epoch 127/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1373 - val_loss: 0.2052\n",
      "Epoch 128/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1359 - val_loss: 0.2100\n",
      "Epoch 129/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1397 - val_loss: 0.2064\n",
      "Epoch 130/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1355 - val_loss: 0.2184\n",
      "Epoch 131/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1354 - val_loss: 0.2044\n",
      "Epoch 132/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1362 - val_loss: 0.2042\n",
      "Epoch 133/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1363 - val_loss: 0.2201\n",
      "Epoch 134/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1364 - val_loss: 0.2117\n",
      "Epoch 135/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1332 - val_loss: 0.2053\n",
      "Epoch 136/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1321 - val_loss: 0.2214\n",
      "Epoch 137/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1328 - val_loss: 0.1998\n",
      "Epoch 138/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1313 - val_loss: 0.2059\n",
      "Epoch 139/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1359 - val_loss: 0.2044\n",
      "Epoch 140/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1355 - val_loss: 0.2074\n",
      "Epoch 141/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1344 - val_loss: 0.2023\n",
      "Epoch 142/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1332 - val_loss: 0.2073\n",
      "Epoch 143/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1353 - val_loss: 0.2011\n",
      "Epoch 144/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1342 - val_loss: 0.2112\n",
      "666\n",
      "Epoch 1/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 2.4661 - val_loss: 1.1754\n",
      "Epoch 2/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.5974 - val_loss: 0.4017\n",
      "Epoch 3/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.3869 - val_loss: 0.3760\n",
      "Epoch 4/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.3514 - val_loss: 0.3474\n",
      "Epoch 5/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.3375 - val_loss: 0.3447\n",
      "Epoch 6/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3184 - val_loss: 0.3228\n",
      "Epoch 7/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3114 - val_loss: 0.2900\n",
      "Epoch 8/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2952 - val_loss: 0.2862\n",
      "Epoch 9/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2903 - val_loss: 0.2964\n",
      "Epoch 10/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2849 - val_loss: 0.3557\n",
      "Epoch 11/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2773 - val_loss: 0.2635\n",
      "Epoch 12/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2706 - val_loss: 0.2649\n",
      "Epoch 13/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2699 - val_loss: 0.2951\n",
      "Epoch 14/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2611 - val_loss: 0.2765\n",
      "Epoch 15/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2575 - val_loss: 0.2725\n",
      "Epoch 16/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2555 - val_loss: 0.2723\n",
      "Epoch 17/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2516 - val_loss: 0.2551\n",
      "Epoch 18/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2429 - val_loss: 0.2767\n",
      "Epoch 19/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.2445 - val_loss: 0.2648\n",
      "Epoch 20/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2379 - val_loss: 0.2589\n",
      "Epoch 21/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.2340 - val_loss: 0.2367\n",
      "Epoch 22/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2344 - val_loss: 0.2514\n",
      "Epoch 23/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2321 - val_loss: 0.2455\n",
      "Epoch 24/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2286 - val_loss: 0.2652\n",
      "Epoch 25/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2275 - val_loss: 0.2561\n",
      "Epoch 26/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2265 - val_loss: 0.2425\n",
      "Epoch 27/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.2227 - val_loss: 0.2394\n",
      "Epoch 28/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2239 - val_loss: 0.2404\n",
      "Epoch 29/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2170 - val_loss: 0.2404\n",
      "Epoch 30/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2181 - val_loss: 0.2373\n",
      "Epoch 31/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2128 - val_loss: 0.2426\n",
      "Epoch 32/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2092 - val_loss: 0.2418\n",
      "Epoch 33/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2095 - val_loss: 0.2301\n",
      "Epoch 34/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2082 - val_loss: 0.2739\n",
      "Epoch 35/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2085 - val_loss: 0.2407\n",
      "Epoch 36/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2041 - val_loss: 0.2347\n",
      "Epoch 37/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2051 - val_loss: 0.2695\n",
      "Epoch 38/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.2005 - val_loss: 0.2305\n",
      "Epoch 39/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2003 - val_loss: 0.2341\n",
      "Epoch 40/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1960 - val_loss: 0.2883\n",
      "Epoch 41/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1945 - val_loss: 0.2340\n",
      "Epoch 42/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1931 - val_loss: 0.2314\n",
      "Epoch 43/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1951 - val_loss: 0.2293\n",
      "Epoch 44/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1923 - val_loss: 0.2272\n",
      "Epoch 45/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1923 - val_loss: 0.2202\n",
      "Epoch 46/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1911 - val_loss: 0.2347\n",
      "Epoch 47/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1874 - val_loss: 0.2289\n",
      "Epoch 48/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1859 - val_loss: 0.2390\n",
      "Epoch 49/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1857 - val_loss: 0.2241\n",
      "Epoch 50/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1848 - val_loss: 0.2311\n",
      "Epoch 51/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1817 - val_loss: 0.2426\n",
      "Epoch 52/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1811 - val_loss: 0.2345\n",
      "Epoch 53/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1813 - val_loss: 0.2232\n",
      "Epoch 54/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1819 - val_loss: 0.2230\n",
      "Epoch 55/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1797 - val_loss: 0.2214\n",
      "Epoch 56/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1781 - val_loss: 0.2380\n",
      "Epoch 57/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1768 - val_loss: 0.2163\n",
      "Epoch 58/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1787 - val_loss: 0.2168\n",
      "Epoch 59/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1754 - val_loss: 0.2499\n",
      "Epoch 60/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1733 - val_loss: 0.2268\n",
      "Epoch 61/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1753 - val_loss: 0.2182\n",
      "Epoch 62/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1739 - val_loss: 0.2187\n",
      "Epoch 63/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1712 - val_loss: 0.2155\n",
      "Epoch 64/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1701 - val_loss: 0.2263\n",
      "Epoch 65/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1700 - val_loss: 0.2550\n",
      "Epoch 66/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1686 - val_loss: 0.2141\n",
      "Epoch 67/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1692 - val_loss: 0.2168\n",
      "Epoch 68/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1680 - val_loss: 0.2201\n",
      "Epoch 69/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1670 - val_loss: 0.2147\n",
      "Epoch 70/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1674 - val_loss: 0.2336\n",
      "Epoch 71/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1651 - val_loss: 0.2294\n",
      "Epoch 72/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1624 - val_loss: 0.2150\n",
      "Epoch 73/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1621 - val_loss: 0.2155\n",
      "Epoch 74/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1638 - val_loss: 0.2337\n",
      "Epoch 75/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1629 - val_loss: 0.2173\n",
      "Epoch 76/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1610 - val_loss: 0.2268\n",
      "Epoch 77/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1608 - val_loss: 0.2213\n",
      "Epoch 78/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1619 - val_loss: 0.2163\n",
      "Epoch 79/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1582 - val_loss: 0.2220\n",
      "Epoch 80/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1621 - val_loss: 0.2201\n",
      "Epoch 81/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1562 - val_loss: 0.2380\n",
      "Epoch 82/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1584 - val_loss: 0.2279\n",
      "Epoch 83/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1572 - val_loss: 0.2129\n",
      "Epoch 84/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1579 - val_loss: 0.2203\n",
      "Epoch 85/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1561 - val_loss: 0.2083\n",
      "Epoch 86/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1555 - val_loss: 0.2166\n",
      "Epoch 87/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1556 - val_loss: 0.2136\n",
      "Epoch 88/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1533 - val_loss: 0.2245\n",
      "Epoch 89/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1526 - val_loss: 0.2145\n",
      "Epoch 90/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1538 - val_loss: 0.2138\n",
      "Epoch 91/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1512 - val_loss: 0.2084\n",
      "Epoch 92/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1509 - val_loss: 0.2150\n",
      "Epoch 93/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1513 - val_loss: 0.2132\n",
      "Epoch 94/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1507 - val_loss: 0.2138\n",
      "Epoch 95/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1502 - val_loss: 0.2361\n",
      "Epoch 96/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1536 - val_loss: 0.2245\n",
      "Epoch 97/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1477 - val_loss: 0.2173\n",
      "Epoch 98/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1513 - val_loss: 0.2123\n",
      "Epoch 99/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1484 - val_loss: 0.2076\n",
      "Epoch 100/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1475 - val_loss: 0.2065\n",
      "Epoch 101/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1495 - val_loss: 0.2181\n",
      "Epoch 102/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1475 - val_loss: 0.2092\n",
      "Epoch 103/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1469 - val_loss: 0.2126\n",
      "Epoch 104/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1483 - val_loss: 0.2095\n",
      "Epoch 105/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1461 - val_loss: 0.2064\n",
      "Epoch 106/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1469 - val_loss: 0.2062\n",
      "Epoch 107/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1446 - val_loss: 0.2099\n",
      "Epoch 108/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1467 - val_loss: 0.2048\n",
      "Epoch 109/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1433 - val_loss: 0.2079\n",
      "Epoch 110/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1452 - val_loss: 0.2336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1417 - val_loss: 0.2103\n",
      "Epoch 112/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1408 - val_loss: 0.2141\n",
      "Epoch 113/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1414 - val_loss: 0.2007\n",
      "Epoch 114/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1392 - val_loss: 0.2121\n",
      "Epoch 115/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1432 - val_loss: 0.2087\n",
      "Epoch 116/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1407 - val_loss: 0.2071\n",
      "Epoch 117/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1417 - val_loss: 0.2061\n",
      "Epoch 118/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1423 - val_loss: 0.2087\n",
      "Epoch 119/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1381 - val_loss: 0.2037\n",
      "Epoch 120/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1395 - val_loss: 0.2072\n",
      "Epoch 121/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1404 - val_loss: 0.2085\n",
      "Epoch 122/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1393 - val_loss: 0.2149\n",
      "Epoch 123/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1384 - val_loss: 0.2036\n",
      "Epoch 124/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1402 - val_loss: 0.2043\n",
      "Epoch 125/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1400 - val_loss: 0.2076\n",
      "Epoch 126/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1382 - val_loss: 0.2085\n",
      "Epoch 127/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1366 - val_loss: 0.2068\n",
      "Epoch 128/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1371 - val_loss: 0.1993\n",
      "Epoch 129/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1349 - val_loss: 0.1983\n",
      "Epoch 130/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1345 - val_loss: 0.2129\n",
      "Epoch 131/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1375 - val_loss: 0.2065\n",
      "Epoch 132/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1374 - val_loss: 0.2066\n",
      "Epoch 133/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1353 - val_loss: 0.1999\n",
      "Epoch 134/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1330 - val_loss: 0.2119\n",
      "Epoch 135/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1338 - val_loss: 0.2064\n",
      "Epoch 136/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1335 - val_loss: 0.2011\n",
      "Epoch 137/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1344 - val_loss: 0.2048\n",
      "Epoch 138/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1326 - val_loss: 0.2066\n",
      "Epoch 139/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1327 - val_loss: 0.2072\n",
      "Epoch 140/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1329 - val_loss: 0.2063\n",
      "Epoch 141/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1341 - val_loss: 0.2033\n",
      "Epoch 142/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1317 - val_loss: 0.2009\n",
      "Epoch 143/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1319 - val_loss: 0.1989\n",
      "Epoch 144/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1300 - val_loss: 0.2082\n",
      "Epoch 145/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1322 - val_loss: 0.2064\n",
      "Epoch 146/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1295 - val_loss: 0.2125\n",
      "Epoch 147/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1319 - val_loss: 0.2043\n",
      "Epoch 148/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1324 - val_loss: 0.2096\n",
      "Epoch 149/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1269 - val_loss: 0.2042\n",
      "Epoch 150/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1303 - val_loss: 0.2139\n",
      "Epoch 151/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1297 - val_loss: 0.2111\n",
      "Epoch 152/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1288 - val_loss: 0.2121\n",
      "Epoch 153/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1291 - val_loss: 0.2093\n",
      "Epoch 154/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1284 - val_loss: 0.2005\n",
      "Epoch 155/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1279 - val_loss: 0.2090\n",
      "Epoch 156/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1277 - val_loss: 0.2039\n",
      "Epoch 157/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1265 - val_loss: 0.2050\n",
      "Epoch 158/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1280 - val_loss: 0.2026\n",
      "Epoch 159/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1281 - val_loss: 0.2014\n",
      "5\n",
      "Epoch 1/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 2.4679 - val_loss: 1.1104\n",
      "Epoch 2/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.5755 - val_loss: 0.3590\n",
      "Epoch 3/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3802 - val_loss: 0.3360\n",
      "Epoch 4/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3497 - val_loss: 0.3143\n",
      "Epoch 5/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.3319 - val_loss: 0.3016\n",
      "Epoch 6/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.3165 - val_loss: 0.3741\n",
      "Epoch 7/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3157 - val_loss: 0.3057\n",
      "Epoch 8/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.3028 - val_loss: 0.2983\n",
      "Epoch 9/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2924 - val_loss: 0.3070\n",
      "Epoch 10/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.2859 - val_loss: 0.2817\n",
      "Epoch 11/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2790 - val_loss: 0.3020\n",
      "Epoch 12/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2782 - val_loss: 0.2861\n",
      "Epoch 13/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2692 - val_loss: 0.2669\n",
      "Epoch 14/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2630 - val_loss: 0.2841\n",
      "Epoch 15/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2605 - val_loss: 0.2725\n",
      "Epoch 16/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2565 - val_loss: 0.2902\n",
      "Epoch 17/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2520 - val_loss: 0.2563\n",
      "Epoch 18/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2517 - val_loss: 0.2635\n",
      "Epoch 19/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2449 - val_loss: 0.2759\n",
      "Epoch 20/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2419 - val_loss: 0.2733\n",
      "Epoch 21/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2372 - val_loss: 0.2556\n",
      "Epoch 22/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2379 - val_loss: 0.2554\n",
      "Epoch 23/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2345 - val_loss: 0.2492\n",
      "Epoch 24/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2326 - val_loss: 0.2573\n",
      "Epoch 25/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2293 - val_loss: 0.2619\n",
      "Epoch 26/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2266 - val_loss: 0.2717\n",
      "Epoch 27/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2233 - val_loss: 0.2636\n",
      "Epoch 28/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2228 - val_loss: 0.2474\n",
      "Epoch 29/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2205 - val_loss: 0.2571\n",
      "Epoch 30/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2145 - val_loss: 0.2390\n",
      "Epoch 31/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2136 - val_loss: 0.2389\n",
      "Epoch 32/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2134 - val_loss: 0.2483\n",
      "Epoch 33/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2113 - val_loss: 0.2467\n",
      "Epoch 34/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2101 - val_loss: 0.2329\n",
      "Epoch 35/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2102 - val_loss: 0.2388\n",
      "Epoch 36/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2054 - val_loss: 0.2453\n",
      "Epoch 37/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2041 - val_loss: 0.2313\n",
      "Epoch 38/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2025 - val_loss: 0.2382\n",
      "Epoch 39/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2034 - val_loss: 0.2338\n",
      "Epoch 40/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1993 - val_loss: 0.2377\n",
      "Epoch 41/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1985 - val_loss: 0.2335\n",
      "Epoch 42/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1969 - val_loss: 0.2290\n",
      "Epoch 43/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1946 - val_loss: 0.2387\n",
      "Epoch 44/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1925 - val_loss: 0.2361\n",
      "Epoch 45/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1916 - val_loss: 0.2296\n",
      "Epoch 46/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1920 - val_loss: 0.2370\n",
      "Epoch 47/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1903 - val_loss: 0.2323\n",
      "Epoch 48/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1914 - val_loss: 0.2391\n",
      "Epoch 49/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1872 - val_loss: 0.2897\n",
      "Epoch 50/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1867 - val_loss: 0.2319\n",
      "Epoch 51/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1849 - val_loss: 0.2356\n",
      "Epoch 52/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1817 - val_loss: 0.2287\n",
      "Epoch 53/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1840 - val_loss: 0.2283\n",
      "Epoch 54/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1814 - val_loss: 0.2256\n",
      "Epoch 55/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1797 - val_loss: 0.2217\n",
      "Epoch 56/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1790 - val_loss: 0.2335\n",
      "Epoch 57/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1782 - val_loss: 0.2309\n",
      "Epoch 58/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1775 - val_loss: 0.2308\n",
      "Epoch 59/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1778 - val_loss: 0.2259\n",
      "Epoch 60/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1758 - val_loss: 0.2311\n",
      "Epoch 61/300\n",
      "1048/1048 [==============================] - 6s 6ms/step - loss: 0.1733 - val_loss: 0.2220\n",
      "Epoch 62/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1762 - val_loss: 0.2280\n",
      "Epoch 63/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1717 - val_loss: 0.2239\n",
      "Epoch 64/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1713 - val_loss: 0.2292\n",
      "Epoch 65/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1700 - val_loss: 0.2173\n",
      "Epoch 66/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1712 - val_loss: 0.2283\n",
      "Epoch 67/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1688 - val_loss: 0.2288\n",
      "Epoch 68/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1699 - val_loss: 0.2321\n",
      "Epoch 69/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1682 - val_loss: 0.2257\n",
      "Epoch 70/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1677 - val_loss: 0.2229\n",
      "Epoch 71/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1660 - val_loss: 0.2163\n",
      "Epoch 72/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1646 - val_loss: 0.2188\n",
      "Epoch 73/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1646 - val_loss: 0.2252\n",
      "Epoch 74/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1652 - val_loss: 0.2298\n",
      "Epoch 75/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1636 - val_loss: 0.2198\n",
      "Epoch 76/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1650 - val_loss: 0.2208\n",
      "Epoch 77/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1601 - val_loss: 0.2176\n",
      "Epoch 78/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1591 - val_loss: 0.2256\n",
      "Epoch 79/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1604 - val_loss: 0.2221\n",
      "Epoch 80/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1604 - val_loss: 0.2297\n",
      "Epoch 81/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1591 - val_loss: 0.2204\n",
      "Epoch 82/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1609 - val_loss: 0.2148\n",
      "Epoch 83/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1573 - val_loss: 0.2195\n",
      "Epoch 84/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1566 - val_loss: 0.2111\n",
      "Epoch 85/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1553 - val_loss: 0.2190\n",
      "Epoch 86/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1555 - val_loss: 0.2152\n",
      "Epoch 87/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1545 - val_loss: 0.2201\n",
      "Epoch 88/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1542 - val_loss: 0.2266\n",
      "Epoch 89/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1554 - val_loss: 0.2378\n",
      "Epoch 90/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1522 - val_loss: 0.2134\n",
      "Epoch 91/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1527 - val_loss: 0.2167\n",
      "Epoch 92/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1535 - val_loss: 0.2100\n",
      "Epoch 93/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1545 - val_loss: 0.2161\n",
      "Epoch 94/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1519 - val_loss: 0.2138\n",
      "Epoch 95/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1501 - val_loss: 0.2158\n",
      "Epoch 96/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1501 - val_loss: 0.2153\n",
      "Epoch 97/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1498 - val_loss: 0.2170\n",
      "Epoch 98/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1505 - val_loss: 0.2556\n",
      "Epoch 99/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1456 - val_loss: 0.2110\n",
      "Epoch 100/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1487 - val_loss: 0.2111\n",
      "Epoch 101/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1486 - val_loss: 0.2125\n",
      "Epoch 102/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1464 - val_loss: 0.2116\n",
      "Epoch 103/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1458 - val_loss: 0.2115\n",
      "Epoch 104/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1463 - val_loss: 0.2140\n",
      "Epoch 105/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1466 - val_loss: 0.2190\n",
      "Epoch 106/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1445 - val_loss: 0.2141\n",
      "Epoch 107/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1437 - val_loss: 0.2245\n",
      "Epoch 108/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1449 - val_loss: 0.2257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1420 - val_loss: 0.2127\n",
      "Epoch 110/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1447 - val_loss: 0.2136\n",
      "Epoch 111/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1417 - val_loss: 0.2252\n",
      "Epoch 112/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1431 - val_loss: 0.2114\n",
      "Epoch 113/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1439 - val_loss: 0.2170\n",
      "Epoch 114/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1437 - val_loss: 0.2178\n",
      "Epoch 115/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1408 - val_loss: 0.2064\n",
      "Epoch 116/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1407 - val_loss: 0.2236\n",
      "Epoch 117/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1394 - val_loss: 0.2138\n",
      "Epoch 118/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1402 - val_loss: 0.2067\n",
      "Epoch 119/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1378 - val_loss: 0.2118\n",
      "Epoch 120/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1377 - val_loss: 0.2089\n",
      "Epoch 121/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1400 - val_loss: 0.2112\n",
      "Epoch 122/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1383 - val_loss: 0.2114\n",
      "Epoch 123/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1387 - val_loss: 0.2168\n",
      "Epoch 124/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1371 - val_loss: 0.2132\n",
      "Epoch 125/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1380 - val_loss: 0.2183\n",
      "Epoch 126/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1376 - val_loss: 0.2068\n",
      "Epoch 127/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1371 - val_loss: 0.2325\n",
      "Epoch 128/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1349 - val_loss: 0.2104\n",
      "Epoch 129/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1371 - val_loss: 0.2401\n",
      "Epoch 130/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1354 - val_loss: 0.2016\n",
      "Epoch 131/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1338 - val_loss: 0.2131\n",
      "Epoch 132/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1359 - val_loss: 0.2153\n",
      "Epoch 133/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1330 - val_loss: 0.2163\n",
      "Epoch 134/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1343 - val_loss: 0.2065\n",
      "Epoch 135/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1324 - val_loss: 0.2339\n",
      "Epoch 136/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1350 - val_loss: 0.2076\n",
      "Epoch 137/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1339 - val_loss: 0.2151\n",
      "Epoch 138/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1323 - val_loss: 0.2083\n",
      "Epoch 139/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1338 - val_loss: 0.2092\n",
      "Epoch 140/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1323 - val_loss: 0.2120\n",
      "Epoch 141/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1322 - val_loss: 0.2045\n",
      "Epoch 142/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1325 - val_loss: 0.2073\n",
      "Epoch 143/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1312 - val_loss: 0.2100\n",
      "Epoch 144/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1303 - val_loss: 0.2003\n",
      "Epoch 145/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1311 - val_loss: 0.2067\n",
      "Epoch 146/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1311 - val_loss: 0.2085\n",
      "Epoch 147/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1310 - val_loss: 0.2088\n",
      "Epoch 148/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1300 - val_loss: 0.2062\n",
      "Epoch 149/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1306 - val_loss: 0.2078\n",
      "Epoch 150/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1280 - val_loss: 0.2075\n",
      "Epoch 151/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1300 - val_loss: 0.2486\n",
      "Epoch 152/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1311 - val_loss: 0.2056\n",
      "Epoch 153/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1283 - val_loss: 0.2132\n",
      "Epoch 154/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1283 - val_loss: 0.2019\n",
      "Epoch 155/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1275 - val_loss: 0.2067\n",
      "Epoch 156/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1275 - val_loss: 0.2080\n",
      "Epoch 157/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1287 - val_loss: 0.2083\n",
      "Epoch 158/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1281 - val_loss: 0.2011\n",
      "Epoch 159/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1257 - val_loss: 0.2022\n",
      "Epoch 160/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1254 - val_loss: 0.2239\n",
      "Epoch 161/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1265 - val_loss: 0.2048\n",
      "Epoch 162/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1265 - val_loss: 0.2066\n",
      "Epoch 163/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1256 - val_loss: 0.2074\n",
      "Epoch 164/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1253 - val_loss: 0.2081\n",
      "Epoch 165/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1260 - val_loss: 0.2067\n",
      "Epoch 166/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1241 - val_loss: 0.2032\n",
      "Epoch 167/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1253 - val_loss: 0.2031\n",
      "Epoch 168/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1241 - val_loss: 0.2092\n",
      "Epoch 169/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1248 - val_loss: 0.2103\n",
      "Epoch 170/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1232 - val_loss: 0.2020\n",
      "Epoch 171/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1240 - val_loss: 0.2016\n",
      "Epoch 172/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1252 - val_loss: 0.2071\n",
      "Epoch 173/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1254 - val_loss: 0.2060\n",
      "Epoch 174/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1231 - val_loss: 0.2049\n",
      "10\n",
      "Epoch 1/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 2.5520 - val_loss: 1.2333\n",
      "Epoch 2/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.5971 - val_loss: 0.3659\n",
      "Epoch 3/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3695 - val_loss: 0.3346\n",
      "Epoch 4/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3386 - val_loss: 0.3113\n",
      "Epoch 5/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3224 - val_loss: 0.3029\n",
      "Epoch 6/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3115 - val_loss: 0.3083\n",
      "Epoch 7/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2991 - val_loss: 0.2789\n",
      "Epoch 8/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2929 - val_loss: 0.2752\n",
      "Epoch 9/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2829 - val_loss: 0.2881\n",
      "Epoch 10/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2781 - val_loss: 0.2673\n",
      "Epoch 11/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2712 - val_loss: 0.2661\n",
      "Epoch 12/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2668 - val_loss: 0.2660\n",
      "Epoch 13/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2638 - val_loss: 0.2609\n",
      "Epoch 14/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2546 - val_loss: 0.2642\n",
      "Epoch 15/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2550 - val_loss: 0.2515\n",
      "Epoch 16/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2495 - val_loss: 0.2555\n",
      "Epoch 17/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2470 - val_loss: 0.2453\n",
      "Epoch 18/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2438 - val_loss: 0.2464\n",
      "Epoch 19/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2415 - val_loss: 0.2416\n",
      "Epoch 20/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2355 - val_loss: 0.2727\n",
      "Epoch 21/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2371 - val_loss: 0.2459\n",
      "Epoch 22/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2328 - val_loss: 0.2392\n",
      "Epoch 23/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2312 - val_loss: 0.2762\n",
      "Epoch 24/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2266 - val_loss: 0.2337\n",
      "Epoch 25/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2246 - val_loss: 0.2390\n",
      "Epoch 26/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2253 - val_loss: 0.2389\n",
      "Epoch 27/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2200 - val_loss: 0.2457\n",
      "Epoch 28/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2167 - val_loss: 0.2399\n",
      "Epoch 29/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2145 - val_loss: 0.2293\n",
      "Epoch 30/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2133 - val_loss: 0.2398\n",
      "Epoch 31/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2152 - val_loss: 0.2350\n",
      "Epoch 32/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2111 - val_loss: 0.2348\n",
      "Epoch 33/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2099 - val_loss: 0.2334\n",
      "Epoch 34/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2081 - val_loss: 0.2280\n",
      "Epoch 35/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2069 - val_loss: 0.2316\n",
      "Epoch 36/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2046 - val_loss: 0.2299\n",
      "Epoch 37/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2008 - val_loss: 0.2229\n",
      "Epoch 38/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1992 - val_loss: 0.2334\n",
      "Epoch 39/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1995 - val_loss: 0.2430\n",
      "Epoch 40/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1967 - val_loss: 0.2295\n",
      "Epoch 41/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1973 - val_loss: 0.2340\n",
      "Epoch 42/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1951 - val_loss: 0.2342\n",
      "Epoch 43/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1964 - val_loss: 0.2209\n",
      "Epoch 44/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1939 - val_loss: 0.2257\n",
      "Epoch 45/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1904 - val_loss: 0.2177\n",
      "Epoch 46/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1920 - val_loss: 0.2237\n",
      "Epoch 47/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1891 - val_loss: 0.2291\n",
      "Epoch 48/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1876 - val_loss: 0.2191\n",
      "Epoch 49/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1871 - val_loss: 0.2197\n",
      "Epoch 50/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1866 - val_loss: 0.2168\n",
      "Epoch 51/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1855 - val_loss: 0.2194\n",
      "Epoch 52/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1839 - val_loss: 0.2190\n",
      "Epoch 53/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1814 - val_loss: 0.2357\n",
      "Epoch 54/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1820 - val_loss: 0.2171\n",
      "Epoch 55/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1816 - val_loss: 0.2173\n",
      "Epoch 56/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1780 - val_loss: 0.2358\n",
      "Epoch 57/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1792 - val_loss: 0.2224\n",
      "Epoch 58/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1782 - val_loss: 0.2276\n",
      "Epoch 59/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1752 - val_loss: 0.2144\n",
      "Epoch 60/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1732 - val_loss: 0.2141\n",
      "Epoch 61/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1749 - val_loss: 0.2312\n",
      "Epoch 62/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1724 - val_loss: 0.2217\n",
      "Epoch 63/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1717 - val_loss: 0.2155\n",
      "Epoch 64/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1713 - val_loss: 0.2094\n",
      "Epoch 65/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1714 - val_loss: 0.2241\n",
      "Epoch 66/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1682 - val_loss: 0.2195\n",
      "Epoch 67/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1674 - val_loss: 0.2078\n",
      "Epoch 68/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1705 - val_loss: 0.2131\n",
      "Epoch 69/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1663 - val_loss: 0.2148\n",
      "Epoch 70/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1680 - val_loss: 0.2110\n",
      "Epoch 71/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1685 - val_loss: 0.2144\n",
      "Epoch 72/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1662 - val_loss: 0.2499\n",
      "Epoch 73/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1646 - val_loss: 0.2149\n",
      "Epoch 74/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1647 - val_loss: 0.2230\n",
      "Epoch 75/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1619 - val_loss: 0.2083\n",
      "Epoch 76/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1630 - val_loss: 0.2222\n",
      "Epoch 77/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1637 - val_loss: 0.2121\n",
      "Epoch 78/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1623 - val_loss: 0.2124\n",
      "Epoch 79/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1613 - val_loss: 0.2213\n",
      "Epoch 80/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1598 - val_loss: 0.2137\n",
      "Epoch 81/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1591 - val_loss: 0.2153\n",
      "Epoch 82/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1600 - val_loss: 0.2093\n",
      "Epoch 83/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1618 - val_loss: 0.2155\n",
      "Epoch 84/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1583 - val_loss: 0.2070\n",
      "Epoch 85/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1555 - val_loss: 0.2076\n",
      "Epoch 86/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1562 - val_loss: 0.2329\n",
      "Epoch 87/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1564 - val_loss: 0.2152\n",
      "Epoch 88/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1549 - val_loss: 0.2102\n",
      "Epoch 89/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1556 - val_loss: 0.2067\n",
      "Epoch 90/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1537 - val_loss: 0.2092\n",
      "Epoch 91/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1545 - val_loss: 0.2074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1557 - val_loss: 0.2052\n",
      "Epoch 93/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1536 - val_loss: 0.2049\n",
      "Epoch 94/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1555 - val_loss: 0.2103\n",
      "Epoch 95/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1520 - val_loss: 0.2073\n",
      "Epoch 96/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1529 - val_loss: 0.2190\n",
      "Epoch 97/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1502 - val_loss: 0.2091\n",
      "Epoch 98/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1522 - val_loss: 0.2103\n",
      "Epoch 99/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1501 - val_loss: 0.2071\n",
      "Epoch 100/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1495 - val_loss: 0.2067\n",
      "Epoch 101/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1507 - val_loss: 0.2119\n",
      "Epoch 102/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1482 - val_loss: 0.2036\n",
      "Epoch 103/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1499 - val_loss: 0.2074\n",
      "Epoch 104/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1474 - val_loss: 0.2055\n",
      "Epoch 105/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1487 - val_loss: 0.2104\n",
      "Epoch 106/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1458 - val_loss: 0.2039\n",
      "Epoch 107/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1492 - val_loss: 0.2006\n",
      "Epoch 108/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1453 - val_loss: 0.2100\n",
      "Epoch 109/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1455 - val_loss: 0.2040\n",
      "Epoch 110/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1447 - val_loss: 0.2073\n",
      "Epoch 111/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1459 - val_loss: 0.2060\n",
      "Epoch 112/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1443 - val_loss: 0.2039\n",
      "Epoch 113/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1423 - val_loss: 0.2046\n",
      "Epoch 114/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1435 - val_loss: 0.2017\n",
      "Epoch 115/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1422 - val_loss: 0.2062\n",
      "Epoch 116/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1421 - val_loss: 0.2085\n",
      "Epoch 117/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1437 - val_loss: 0.2233\n",
      "Epoch 118/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1427 - val_loss: 0.2037\n",
      "Epoch 119/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1408 - val_loss: 0.2032\n",
      "Epoch 120/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1420 - val_loss: 0.2046\n",
      "Epoch 121/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1412 - val_loss: 0.2063\n",
      "Epoch 122/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1406 - val_loss: 0.2019\n",
      "Epoch 123/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1403 - val_loss: 0.2115\n",
      "Epoch 124/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1405 - val_loss: 0.2041\n",
      "Epoch 125/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1383 - val_loss: 0.2017\n",
      "Epoch 126/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1391 - val_loss: 0.2025\n",
      "Epoch 127/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1365 - val_loss: 0.1974\n",
      "Epoch 128/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1365 - val_loss: 0.2075\n",
      "Epoch 129/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1374 - val_loss: 0.2106\n",
      "Epoch 130/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1347 - val_loss: 0.2035\n",
      "Epoch 131/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1355 - val_loss: 0.2008\n",
      "Epoch 132/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1361 - val_loss: 0.1953\n",
      "Epoch 133/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1372 - val_loss: 0.1990\n",
      "Epoch 134/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1363 - val_loss: 0.2002\n",
      "Epoch 135/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1357 - val_loss: 0.2011\n",
      "Epoch 136/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1358 - val_loss: 0.2048\n",
      "Epoch 137/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1354 - val_loss: 0.2010\n",
      "Epoch 138/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1355 - val_loss: 0.2035\n",
      "Epoch 139/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1365 - val_loss: 0.2023\n",
      "Epoch 140/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1355 - val_loss: 0.1982\n",
      "Epoch 141/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1361 - val_loss: 0.2024\n",
      "Epoch 142/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1341 - val_loss: 0.1994\n",
      "Epoch 143/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1344 - val_loss: 0.2002\n",
      "Epoch 144/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1321 - val_loss: 0.2002\n",
      "Epoch 145/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1308 - val_loss: 0.2019\n",
      "Epoch 146/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1334 - val_loss: 0.2111\n",
      "Epoch 147/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1336 - val_loss: 0.2077\n",
      "Epoch 148/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1312 - val_loss: 0.2056\n",
      "Epoch 149/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1327 - val_loss: 0.2072\n",
      "Epoch 150/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1337 - val_loss: 0.1955\n",
      "Epoch 151/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1328 - val_loss: 0.1966\n",
      "Epoch 152/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1310 - val_loss: 0.1990\n",
      "Epoch 153/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1292 - val_loss: 0.2044\n",
      "Epoch 154/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1318 - val_loss: 0.1992\n",
      "Epoch 155/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1335 - val_loss: 0.1989\n",
      "Epoch 156/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1305 - val_loss: 0.2132\n",
      "Epoch 157/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1300 - val_loss: 0.2049\n",
      "Epoch 158/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1297 - val_loss: 0.1971\n",
      "Epoch 159/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1286 - val_loss: 0.1993\n",
      "Epoch 160/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1298 - val_loss: 0.2035\n",
      "Epoch 161/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1280 - val_loss: 0.2019\n",
      "Epoch 162/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1321 - val_loss: 0.2126\n",
      "1993\n",
      "Epoch 1/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 2.5249 - val_loss: 1.1235\n",
      "Epoch 2/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.6071 - val_loss: 0.3844\n",
      "Epoch 3/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3771 - val_loss: 0.3378\n",
      "Epoch 4/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3431 - val_loss: 0.3616\n",
      "Epoch 5/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3253 - val_loss: 0.2923\n",
      "Epoch 6/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3117 - val_loss: 0.3085\n",
      "Epoch 7/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.3051 - val_loss: 0.2888\n",
      "Epoch 8/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2964 - val_loss: 0.2801\n",
      "Epoch 9/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2889 - val_loss: 0.2699\n",
      "Epoch 10/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2798 - val_loss: 0.2787\n",
      "Epoch 11/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2763 - val_loss: 0.2606\n",
      "Epoch 12/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2702 - val_loss: 0.2841\n",
      "Epoch 13/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2669 - val_loss: 0.2802\n",
      "Epoch 14/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2624 - val_loss: 0.2719\n",
      "Epoch 15/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2600 - val_loss: 0.2598\n",
      "Epoch 16/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2551 - val_loss: 0.2762\n",
      "Epoch 17/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2487 - val_loss: 0.2456\n",
      "Epoch 18/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2467 - val_loss: 0.2552\n",
      "Epoch 19/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2436 - val_loss: 0.2538\n",
      "Epoch 20/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2399 - val_loss: 0.2565\n",
      "Epoch 21/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2373 - val_loss: 0.2482\n",
      "Epoch 22/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2357 - val_loss: 0.2495\n",
      "Epoch 23/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2328 - val_loss: 0.2431\n",
      "Epoch 24/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2306 - val_loss: 0.2387\n",
      "Epoch 25/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2270 - val_loss: 0.2361\n",
      "Epoch 26/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2257 - val_loss: 0.2358\n",
      "Epoch 27/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2227 - val_loss: 0.2405\n",
      "Epoch 28/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2224 - val_loss: 0.2578\n",
      "Epoch 29/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2188 - val_loss: 0.2431\n",
      "Epoch 30/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.2188 - val_loss: 0.2395\n",
      "Epoch 31/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2143 - val_loss: 0.2791\n",
      "Epoch 32/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2116 - val_loss: 0.2299\n",
      "Epoch 33/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2102 - val_loss: 0.2469\n",
      "Epoch 34/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2092 - val_loss: 0.2560\n",
      "Epoch 35/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2070 - val_loss: 0.2419\n",
      "Epoch 36/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2077 - val_loss: 0.2268\n",
      "Epoch 37/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2055 - val_loss: 0.2256\n",
      "Epoch 38/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2025 - val_loss: 0.2330\n",
      "Epoch 39/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2016 - val_loss: 0.2373\n",
      "Epoch 40/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2004 - val_loss: 0.2294\n",
      "Epoch 41/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1989 - val_loss: 0.2331\n",
      "Epoch 42/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1989 - val_loss: 0.2682\n",
      "Epoch 43/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1966 - val_loss: 0.2339\n",
      "Epoch 44/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1953 - val_loss: 0.2446\n",
      "Epoch 45/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1944 - val_loss: 0.2293\n",
      "Epoch 46/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1948 - val_loss: 0.2182\n",
      "Epoch 47/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1882 - val_loss: 0.2242\n",
      "Epoch 48/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1899 - val_loss: 0.2232\n",
      "Epoch 49/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1881 - val_loss: 0.2349\n",
      "Epoch 50/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1880 - val_loss: 0.2190\n",
      "Epoch 51/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1866 - val_loss: 0.2278\n",
      "Epoch 52/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1838 - val_loss: 0.2402\n",
      "Epoch 53/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1826 - val_loss: 0.2325\n",
      "Epoch 54/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1833 - val_loss: 0.2311\n",
      "Epoch 55/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1800 - val_loss: 0.2220\n",
      "Epoch 56/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1819 - val_loss: 0.2267\n",
      "Epoch 57/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1821 - val_loss: 0.2285\n",
      "Epoch 58/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1770 - val_loss: 0.2336\n",
      "Epoch 59/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1777 - val_loss: 0.2167\n",
      "Epoch 60/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1758 - val_loss: 0.2168\n",
      "Epoch 61/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1760 - val_loss: 0.2140\n",
      "Epoch 62/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1745 - val_loss: 0.2267\n",
      "Epoch 63/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1738 - val_loss: 0.2211\n",
      "Epoch 64/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1743 - val_loss: 0.2436\n",
      "Epoch 65/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1713 - val_loss: 0.2286\n",
      "Epoch 66/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1723 - val_loss: 0.2190\n",
      "Epoch 67/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1736 - val_loss: 0.2141\n",
      "Epoch 68/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1688 - val_loss: 0.2134\n",
      "Epoch 69/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1675 - val_loss: 0.2251\n",
      "Epoch 70/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1701 - val_loss: 0.2107\n",
      "Epoch 71/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1695 - val_loss: 0.2143\n",
      "Epoch 72/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1672 - val_loss: 0.2114\n",
      "Epoch 73/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1685 - val_loss: 0.2158\n",
      "Epoch 74/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1668 - val_loss: 0.2198\n",
      "Epoch 75/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1645 - val_loss: 0.2206\n",
      "Epoch 76/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1618 - val_loss: 0.2118\n",
      "Epoch 77/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1658 - val_loss: 0.2158\n",
      "Epoch 78/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1616 - val_loss: 0.2149\n",
      "Epoch 79/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1616 - val_loss: 0.2096\n",
      "Epoch 80/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1616 - val_loss: 0.2077\n",
      "Epoch 81/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1616 - val_loss: 0.2124\n",
      "Epoch 82/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1588 - val_loss: 0.2149\n",
      "Epoch 83/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1590 - val_loss: 0.2161\n",
      "Epoch 84/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1617 - val_loss: 0.2164\n",
      "Epoch 85/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1579 - val_loss: 0.2149\n",
      "Epoch 86/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1575 - val_loss: 0.2138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1593 - val_loss: 0.2112\n",
      "Epoch 88/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1558 - val_loss: 0.2608\n",
      "Epoch 89/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1549 - val_loss: 0.2502\n",
      "Epoch 90/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1561 - val_loss: 0.2117\n",
      "Epoch 91/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1552 - val_loss: 0.2117\n",
      "Epoch 92/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1542 - val_loss: 0.2169\n",
      "Epoch 93/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1525 - val_loss: 0.2086\n",
      "Epoch 94/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1521 - val_loss: 0.2134\n",
      "Epoch 95/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1523 - val_loss: 0.2075\n",
      "Epoch 96/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1537 - val_loss: 0.2157\n",
      "Epoch 97/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1502 - val_loss: 0.2047\n",
      "Epoch 98/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1526 - val_loss: 0.2133\n",
      "Epoch 99/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1506 - val_loss: 0.2112\n",
      "Epoch 100/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1526 - val_loss: 0.2165\n",
      "Epoch 101/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1526 - val_loss: 0.2068\n",
      "Epoch 102/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1494 - val_loss: 0.2078\n",
      "Epoch 103/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1508 - val_loss: 0.2094\n",
      "Epoch 104/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1481 - val_loss: 0.2150\n",
      "Epoch 105/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1485 - val_loss: 0.2110\n",
      "Epoch 106/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1468 - val_loss: 0.2109\n",
      "Epoch 107/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1454 - val_loss: 0.2377\n",
      "Epoch 108/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1440 - val_loss: 0.2140\n",
      "Epoch 109/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1456 - val_loss: 0.2062\n",
      "Epoch 110/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1447 - val_loss: 0.2055\n",
      "Epoch 111/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1438 - val_loss: 0.2212\n",
      "Epoch 112/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1455 - val_loss: 0.2075\n",
      "Epoch 113/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1416 - val_loss: 0.2066\n",
      "Epoch 114/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1416 - val_loss: 0.2040\n",
      "Epoch 115/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1426 - val_loss: 0.2032\n",
      "Epoch 116/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1427 - val_loss: 0.2079\n",
      "Epoch 117/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1412 - val_loss: 0.2067\n",
      "Epoch 118/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1410 - val_loss: 0.2071\n",
      "Epoch 119/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1401 - val_loss: 0.2075\n",
      "Epoch 120/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1412 - val_loss: 0.2129\n",
      "Epoch 121/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1411 - val_loss: 0.2029\n",
      "Epoch 122/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1422 - val_loss: 0.2082\n",
      "Epoch 123/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1404 - val_loss: 0.2027\n",
      "Epoch 124/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1390 - val_loss: 0.2056\n",
      "Epoch 125/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1387 - val_loss: 0.2100\n",
      "Epoch 126/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1401 - val_loss: 0.2029\n",
      "Epoch 127/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1371 - val_loss: 0.2041\n",
      "Epoch 128/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1346 - val_loss: 0.1998\n",
      "Epoch 129/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1375 - val_loss: 0.2062\n",
      "Epoch 130/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1366 - val_loss: 0.2067\n",
      "Epoch 131/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1364 - val_loss: 0.2120\n",
      "Epoch 132/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1368 - val_loss: 0.2070\n",
      "Epoch 133/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1351 - val_loss: 0.2027\n",
      "Epoch 134/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1347 - val_loss: 0.2104\n",
      "Epoch 135/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1358 - val_loss: 0.2003\n",
      "Epoch 136/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1340 - val_loss: 0.1993\n",
      "Epoch 137/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1335 - val_loss: 0.2132\n",
      "Epoch 138/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1338 - val_loss: 0.2075\n",
      "Epoch 139/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1310 - val_loss: 0.2050\n",
      "Epoch 140/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1344 - val_loss: 0.2007\n",
      "Epoch 141/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1330 - val_loss: 0.2020\n",
      "Epoch 142/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1322 - val_loss: 0.2075\n",
      "Epoch 143/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1339 - val_loss: 0.1998\n",
      "Epoch 144/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1332 - val_loss: 0.2031\n",
      "Epoch 145/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1324 - val_loss: 0.2131\n",
      "Epoch 146/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1329 - val_loss: 0.2056\n",
      "Epoch 147/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1322 - val_loss: 0.2032\n",
      "Epoch 148/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1335 - val_loss: 0.1979\n",
      "Epoch 149/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1300 - val_loss: 0.2002\n",
      "Epoch 150/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1313 - val_loss: 0.2120\n",
      "Epoch 151/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1296 - val_loss: 0.1972\n",
      "Epoch 152/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1282 - val_loss: 0.2047\n",
      "Epoch 153/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1298 - val_loss: 0.2009\n",
      "Epoch 154/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1264 - val_loss: 0.2048\n",
      "Epoch 155/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1304 - val_loss: 0.1989\n",
      "Epoch 156/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1298 - val_loss: 0.1977\n",
      "Epoch 157/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1297 - val_loss: 0.2017\n",
      "Epoch 158/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1299 - val_loss: 0.2016\n",
      "Epoch 159/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1295 - val_loss: 0.2066\n",
      "Epoch 160/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1301 - val_loss: 0.1996\n",
      "Epoch 161/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1271 - val_loss: 0.2019\n",
      "Epoch 162/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1254 - val_loss: 0.2027\n",
      "Epoch 163/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1272 - val_loss: 0.2002\n",
      "Epoch 164/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1257 - val_loss: 0.2005\n",
      "Epoch 165/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1258 - val_loss: 0.2007\n",
      "Epoch 166/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1272 - val_loss: 0.1980\n",
      "Epoch 167/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1262 - val_loss: 0.2028\n",
      "Epoch 168/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1264 - val_loss: 0.2119\n",
      "Epoch 169/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1270 - val_loss: 0.2019\n",
      "Epoch 170/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1259 - val_loss: 0.2051\n",
      "Epoch 171/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1231 - val_loss: 0.1983\n",
      "Epoch 172/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1257 - val_loss: 0.2052\n",
      "Epoch 173/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1234 - val_loss: 0.2019\n",
      "Epoch 174/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1261 - val_loss: 0.2078\n",
      "Epoch 175/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1238 - val_loss: 0.2084\n",
      "Epoch 176/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1232 - val_loss: 0.2019\n",
      "Epoch 177/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1236 - val_loss: 0.1991\n",
      "Epoch 178/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1253 - val_loss: 0.2007\n",
      "Epoch 179/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1246 - val_loss: 0.2112\n",
      "Epoch 180/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1245 - val_loss: 0.2015\n",
      "Epoch 181/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1233 - val_loss: 0.2019\n",
      "13\n",
      "Epoch 1/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 2.4622 - val_loss: 1.2867\n",
      "Epoch 2/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.5895 - val_loss: 0.3907\n",
      "Epoch 3/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3888 - val_loss: 0.3486\n",
      "Epoch 4/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3554 - val_loss: 0.3136\n",
      "Epoch 5/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3320 - val_loss: 0.3120\n",
      "Epoch 6/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3183 - val_loss: 0.3064\n",
      "Epoch 7/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.3045 - val_loss: 0.2891\n",
      "Epoch 8/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2997 - val_loss: 0.2955\n",
      "Epoch 9/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2901 - val_loss: 0.2991\n",
      "Epoch 10/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2870 - val_loss: 0.2724\n",
      "Epoch 11/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2769 - val_loss: 0.2781\n",
      "Epoch 12/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2710 - val_loss: 0.2730\n",
      "Epoch 13/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2674 - val_loss: 0.2651\n",
      "Epoch 14/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.2606 - val_loss: 0.2681\n",
      "Epoch 15/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2563 - val_loss: 0.2613\n",
      "Epoch 16/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2546 - val_loss: 0.2669\n",
      "Epoch 17/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2504 - val_loss: 0.2687\n",
      "Epoch 18/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2458 - val_loss: 0.2561\n",
      "Epoch 19/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2459 - val_loss: 0.2645\n",
      "Epoch 20/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2379 - val_loss: 0.2547\n",
      "Epoch 21/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2364 - val_loss: 0.2893\n",
      "Epoch 22/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2336 - val_loss: 0.2472\n",
      "Epoch 23/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2298 - val_loss: 0.2613\n",
      "Epoch 24/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2288 - val_loss: 0.2572\n",
      "Epoch 25/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2274 - val_loss: 0.2481\n",
      "Epoch 26/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2264 - val_loss: 0.2581\n",
      "Epoch 27/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2202 - val_loss: 0.2439\n",
      "Epoch 28/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2204 - val_loss: 0.2419\n",
      "Epoch 29/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2161 - val_loss: 0.2641\n",
      "Epoch 30/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2150 - val_loss: 0.2401\n",
      "Epoch 31/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2125 - val_loss: 0.2343\n",
      "Epoch 32/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2143 - val_loss: 0.2438\n",
      "Epoch 33/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2102 - val_loss: 0.2355\n",
      "Epoch 34/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2072 - val_loss: 0.2339\n",
      "Epoch 35/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.2076 - val_loss: 0.2334\n",
      "Epoch 36/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2061 - val_loss: 0.2776\n",
      "Epoch 37/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2026 - val_loss: 0.2321\n",
      "Epoch 38/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.2035 - val_loss: 0.2673\n",
      "Epoch 39/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1977 - val_loss: 0.2320\n",
      "Epoch 40/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1991 - val_loss: 0.2368\n",
      "Epoch 41/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1992 - val_loss: 0.2248\n",
      "Epoch 42/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1938 - val_loss: 0.2339\n",
      "Epoch 43/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1931 - val_loss: 0.2251\n",
      "Epoch 44/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1957 - val_loss: 0.2332\n",
      "Epoch 45/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1903 - val_loss: 0.2370\n",
      "Epoch 46/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1909 - val_loss: 0.2397\n",
      "Epoch 47/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1901 - val_loss: 0.2248\n",
      "Epoch 48/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1892 - val_loss: 0.2387\n",
      "Epoch 49/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1859 - val_loss: 0.2402\n",
      "Epoch 50/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1876 - val_loss: 0.2280\n",
      "Epoch 51/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1868 - val_loss: 0.2266\n",
      "Epoch 52/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1837 - val_loss: 0.2198\n",
      "Epoch 53/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1812 - val_loss: 0.2240\n",
      "Epoch 54/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1837 - val_loss: 0.2251\n",
      "Epoch 55/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1828 - val_loss: 0.2242\n",
      "Epoch 56/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1818 - val_loss: 0.2158\n",
      "Epoch 57/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1773 - val_loss: 0.2225\n",
      "Epoch 58/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1766 - val_loss: 0.2307\n",
      "Epoch 59/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1752 - val_loss: 0.2407\n",
      "Epoch 60/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1760 - val_loss: 0.2167\n",
      "Epoch 61/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1755 - val_loss: 0.2264\n",
      "Epoch 62/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1772 - val_loss: 0.2238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1716 - val_loss: 0.2288\n",
      "Epoch 64/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1714 - val_loss: 0.2254\n",
      "Epoch 65/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1740 - val_loss: 0.2229\n",
      "Epoch 66/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1708 - val_loss: 0.2174\n",
      "Epoch 67/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1710 - val_loss: 0.2139\n",
      "Epoch 68/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1692 - val_loss: 0.2229\n",
      "Epoch 69/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1695 - val_loss: 0.2481\n",
      "Epoch 70/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1672 - val_loss: 0.2163\n",
      "Epoch 71/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1662 - val_loss: 0.2140\n",
      "Epoch 72/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1656 - val_loss: 0.2138\n",
      "Epoch 73/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1675 - val_loss: 0.2283\n",
      "Epoch 74/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1618 - val_loss: 0.2130\n",
      "Epoch 75/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1634 - val_loss: 0.2223\n",
      "Epoch 76/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1640 - val_loss: 0.2149\n",
      "Epoch 77/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1631 - val_loss: 0.2126\n",
      "Epoch 78/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1603 - val_loss: 0.2249\n",
      "Epoch 79/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1625 - val_loss: 0.2193\n",
      "Epoch 80/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1593 - val_loss: 0.2094\n",
      "Epoch 81/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1605 - val_loss: 0.2188\n",
      "Epoch 82/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1609 - val_loss: 0.2158\n",
      "Epoch 83/300\n",
      "1048/1048 [==============================] - 5s 5ms/step - loss: 0.1598 - val_loss: 0.2102\n",
      "Epoch 84/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1573 - val_loss: 0.2159\n",
      "Epoch 85/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1610 - val_loss: 0.2068\n",
      "Epoch 86/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1563 - val_loss: 0.2053\n",
      "Epoch 87/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1564 - val_loss: 0.2289\n",
      "Epoch 88/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1540 - val_loss: 0.2304\n",
      "Epoch 89/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1532 - val_loss: 0.2046\n",
      "Epoch 90/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1550 - val_loss: 0.2022\n",
      "Epoch 91/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1532 - val_loss: 0.2073\n",
      "Epoch 92/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1570 - val_loss: 0.2143\n",
      "Epoch 93/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1493 - val_loss: 0.2108\n",
      "Epoch 94/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1513 - val_loss: 0.2084\n",
      "Epoch 95/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1505 - val_loss: 0.2279\n",
      "Epoch 96/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1533 - val_loss: 0.2069\n",
      "Epoch 97/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1512 - val_loss: 0.2057\n",
      "Epoch 98/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1500 - val_loss: 0.2061\n",
      "Epoch 99/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1507 - val_loss: 0.2081\n",
      "Epoch 100/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1488 - val_loss: 0.2156\n",
      "Epoch 101/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1476 - val_loss: 0.2092\n",
      "Epoch 102/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1480 - val_loss: 0.2138\n",
      "Epoch 103/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1504 - val_loss: 0.2100\n",
      "Epoch 104/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1493 - val_loss: 0.2490\n",
      "Epoch 105/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1469 - val_loss: 0.2050\n",
      "Epoch 106/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1466 - val_loss: 0.2071\n",
      "Epoch 107/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1443 - val_loss: 0.2131\n",
      "Epoch 108/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1467 - val_loss: 0.2088\n",
      "Epoch 109/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1459 - val_loss: 0.2097\n",
      "Epoch 110/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1454 - val_loss: 0.2122\n",
      "Epoch 111/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1454 - val_loss: 0.2010\n",
      "Epoch 112/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1446 - val_loss: 0.2084\n",
      "Epoch 113/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1436 - val_loss: 0.2054\n",
      "Epoch 114/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1428 - val_loss: 0.2102\n",
      "Epoch 115/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1423 - val_loss: 0.2035\n",
      "Epoch 116/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1421 - val_loss: 0.2152\n",
      "Epoch 117/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1425 - val_loss: 0.2045\n",
      "Epoch 118/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1420 - val_loss: 0.2028\n",
      "Epoch 119/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1417 - val_loss: 0.1993\n",
      "Epoch 120/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1403 - val_loss: 0.2126\n",
      "Epoch 121/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1403 - val_loss: 0.2034\n",
      "Epoch 122/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1407 - val_loss: 0.2021\n",
      "Epoch 123/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1400 - val_loss: 0.2015\n",
      "Epoch 124/300\n",
      "1048/1048 [==============================] - 5s 4ms/step - loss: 0.1388 - val_loss: 0.2066\n",
      "Epoch 125/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1403 - val_loss: 0.2017\n",
      "Epoch 126/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1378 - val_loss: 0.2034\n",
      "Epoch 127/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1376 - val_loss: 0.2024\n",
      "Epoch 128/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1370 - val_loss: 0.2031\n",
      "Epoch 129/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1372 - val_loss: 0.2074\n",
      "Epoch 130/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1382 - val_loss: 0.2040\n",
      "Epoch 131/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1370 - val_loss: 0.2026\n",
      "Epoch 132/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1346 - val_loss: 0.2059\n",
      "Epoch 133/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1324 - val_loss: 0.2044\n",
      "Epoch 134/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1352 - val_loss: 0.2049\n",
      "Epoch 135/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1333 - val_loss: 0.2044\n",
      "Epoch 136/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1356 - val_loss: 0.2042\n",
      "Epoch 137/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1338 - val_loss: 0.2003\n",
      "Epoch 138/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1317 - val_loss: 0.2021\n",
      "Epoch 139/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1355 - val_loss: 0.2031\n",
      "Epoch 140/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1351 - val_loss: 0.2020\n",
      "Epoch 141/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1318 - val_loss: 0.2000\n",
      "Epoch 142/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1336 - val_loss: 0.2127\n",
      "Epoch 143/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1336 - val_loss: 0.2076\n",
      "Epoch 144/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1314 - val_loss: 0.2020\n",
      "Epoch 145/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1315 - val_loss: 0.2253\n",
      "Epoch 146/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1332 - val_loss: 0.2052\n",
      "Epoch 147/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1329 - val_loss: 0.2025\n",
      "Epoch 148/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1300 - val_loss: 0.2111\n",
      "Epoch 149/300\n",
      "1048/1048 [==============================] - 4s 4ms/step - loss: 0.1299 - val_loss: 0.2096\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "rmse_train = []\n",
    "rmse_valid = []\n",
    "rmse_test = []\n",
    "\n",
    "acc_train = []\n",
    "acc_valid = []\n",
    "acc_test = []\n",
    "\n",
    "\n",
    "\n",
    "for i in seeds:\n",
    "    target_and_input_OpenLandMap = pd.read_csv(\"Target_and_input.csv\")\n",
    "    target_and_input_OpenLandMap = target_and_input_OpenLandMap[target_and_input_OpenLandMap.olc_id != \"5G55HPG4+MM7\"]\n",
    "    input_Sentinel_2 = pd.read_csv(\"pixels_df.csv\")\n",
    "    all_data = target_and_input_OpenLandMap.merge(input_Sentinel_2, on='olc_id', how='left')\n",
    "    all_data.dropna(subset=['oc'], inplace=True)\n",
    "    all_data.drop(['olc_id','confidence_degree','uuid','site_obsdate','source_db','layer_sequence.f','hzn_top', 'hzn_bot', 'n_tot', 'ph_h2o'], axis=1, inplace=True)\n",
    "    all_data = all_data[all_data['oc'] < 120]  \n",
    "    train, validate, test = \\\n",
    "              np.split(all_data.sample(frac=1, random_state=i), \n",
    "                       [int(.6*len(all_data)), int(.8*len(all_data))])\n",
    "    \n",
    "    train_targets = train[\"oc\"]\n",
    "    validate_targets = validate[\"oc\"]\n",
    "    test_targets = test[\"oc\"]\n",
    "    \n",
    "    train_inputs = train.drop(['oc'], axis=1)\n",
    "    validate_inputs = validate.drop(['oc'], axis=1)\n",
    "    test_inputs = test.drop(['oc'], axis=1)\n",
    "    \n",
    "    train_inputs.fillna(0, inplace = True)\n",
    "    validate_inputs.fillna(0, inplace = True)\n",
    "    test_inputs.fillna(0, inplace = True)\n",
    "    \n",
    "    min_values = train_inputs.min(axis=0)\n",
    "    max_values = train_inputs.max(axis=0)\n",
    "    \n",
    "    train_inputs=(train_inputs-min_values)/(max_values-min_values)\n",
    "    validate_inputs=(validate_inputs-min_values)/(max_values-min_values)\n",
    "    test_inputs=(test_inputs-min_values)/(max_values-min_values)\n",
    "    \n",
    "    train_targets = np.log(train_targets+1)\n",
    "    validate_targets = np.log(validate_targets+1)\n",
    "    test_targets = np.log(test_targets+1)\n",
    "    \n",
    "    train_inputs = train_inputs.to_numpy()\n",
    "    validate_inputs = validate_inputs.to_numpy()\n",
    "    test_inputs = test_inputs.to_numpy()\n",
    "    \n",
    "    train_targets = train_targets.to_numpy()\n",
    "    validate_targets = validate_targets.to_numpy()\n",
    "    test_targets = test_targets.to_numpy()\n",
    "    \n",
    "    model = K.models.Sequential()\n",
    "    model.add(K.Input(shape=(108,)))\n",
    "    model.add(K.layers.Dense(512, activation='relu'))\n",
    "    #model.add(K.layers.Dropout(0.05))\n",
    "    model.add(K.layers.BatchNormalization())\n",
    "    model.add(K.layers.Dense(256, activation='relu'))\n",
    "    #model.add(K.layers.Dropout(0.05))\n",
    "    model.add(K.layers.BatchNormalization())\n",
    "    model.add(K.layers.Dense(128, activation='relu'))\n",
    "    #model.add(K.layers.Dropout(0.05))\n",
    "    model.add(K.layers.BatchNormalization())\n",
    "    model.add(K.layers.Dense(64, activation='relu'))\n",
    "    #model.add(K.layers.Dropout(0.05))\n",
    "    model.add(K.layers.BatchNormalization())\n",
    "    model.add(K.layers.Dense(1, activation='linear'))\n",
    "    \n",
    "    batch_siz = 32\n",
    "    num_epochs = 300\n",
    "    callback = K.callbacks.EarlyStopping(monitor='val_loss', patience=30, restore_best_weights = True)\n",
    "    \n",
    "    model.compile(loss='mse',\n",
    "              optimizer=K.optimizers.Adam(learning_rate=0.0001)\n",
    "             )\n",
    "    \n",
    "    history = model.fit(train_inputs,train_targets,\n",
    "                    validation_data=(validate_inputs,validate_targets),\n",
    "                    epochs=num_epochs,\n",
    "                    callbacks=[callback])\n",
    "    \n",
    "    \n",
    "    train_predictions = pd.DataFrame(model.predict(train_inputs))\n",
    "    train_df = pd.DataFrame(train_targets)\n",
    "    train_predictions.clip(lower=0, inplace=True)\n",
    "    \n",
    "    rmse_train.append(np.sqrt(np.square(train_predictions - train_df).sum(axis=0)/train_df.shape[0])[0])\n",
    "    acc_train.append(((np.round(train_predictions) == np.round(train_df)).sum(axis=0)/train_df.shape[0])[0])\n",
    "    \n",
    "    validate_predictions = pd.DataFrame(model.predict(validate_inputs))\n",
    "    validate_df = pd.DataFrame(validate_targets)\n",
    "    validate_predictions.clip(lower=0, inplace=True)\n",
    "    \n",
    "    rmse_valid.append(np.sqrt(np.square(validate_predictions - validate_df).sum(axis=0)/validate_df.shape[0])[0])\n",
    "    acc_valid.append(((np.round(validate_predictions) == np.round(validate_df)).sum(axis=0)/validate_df.shape[0])[0])\n",
    "    \n",
    "    test_predictions =pd.DataFrame(model.predict(test_inputs))\n",
    "    test_df = pd.DataFrame(test_targets)\n",
    "    test_predictions.clip(lower=0, inplace=True)\n",
    "    \n",
    "    rmse_test.append(np.sqrt(np.square(test_predictions - test_df).sum(axis=0)/test_df.shape[0])[0])\n",
    "    acc_test.append(((np.round(test_predictions) == np.round(test_df)).sum(axis=0)/test_df.shape[0])[0])\n",
    "    \n",
    "    model.save('Models_eval_as_in_paper/model_without_images_regression_seed_'+str(i)+'.h5')\n",
    "    \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1bc4bcd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.33029161217455777,\n",
       " 0.2975469984768751,\n",
       " 0.28819609199989105,\n",
       " 0.3029086318817659,\n",
       " 0.3043515483418198,\n",
       " 0.29794488513707507,\n",
       " 0.29190854563085217,\n",
       " 0.29571383467274004,\n",
       " 0.29392293460051494,\n",
       " 0.3074806478213045]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "90f286c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7620410962989472,\n",
       " 0.7870627180817751,\n",
       " 0.7895976857236587,\n",
       " 0.7783841818019146,\n",
       " 0.7772807252989771,\n",
       " 0.7842295189526111,\n",
       " 0.7889713996003698,\n",
       " 0.7865557245533984,\n",
       " 0.7874802421639677,\n",
       " 0.7744773493185411]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "170ef627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4476343573829571,\n",
       " 0.4445612574383707,\n",
       " 0.44677419359572274,\n",
       " 0.44380444444039724,\n",
       " 0.446394896295985,\n",
       " 0.4449875464062585,\n",
       " 0.4472438343062849,\n",
       " 0.44170163603920726,\n",
       " 0.44388684483460283,\n",
       " 0.4460656326534337]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7686d745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6830097521696341,\n",
       " 0.6914198801109421,\n",
       " 0.686409591124631,\n",
       " 0.6903462467567326,\n",
       " 0.6870358772479198,\n",
       " 0.685693835555158,\n",
       " 0.6958933524201485,\n",
       " 0.6930303301422565,\n",
       " 0.6902567773105485,\n",
       " 0.6860517133398945]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "55089a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.45975335389501587,\n",
       " 0.44332743102471567,\n",
       " 0.4413642957996906,\n",
       " 0.4433037574544462,\n",
       " 0.44356387337047104,\n",
       " 0.4368064361093876,\n",
       " 0.440301451142748,\n",
       " 0.4428163527140393,\n",
       " 0.44220878540134734,\n",
       " 0.4575938247322753]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "53ddea05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6840833855238436,\n",
       " 0.6860517133398945,\n",
       " 0.6866779994631833,\n",
       " 0.6838149771852913,\n",
       " 0.695535474635412,\n",
       " 0.6919566967880468,\n",
       " 0.694819719065939,\n",
       " 0.6924935134651516,\n",
       " 0.6905251856491008,\n",
       " 0.680862485461215]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e0bd4d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_errors = pd.DataFrame({'seeds':seeds,\n",
    "                               'rmse_train': rmse_train, 'acc_train': acc_train,\n",
    "                              'rmse_valid': rmse_valid, 'acc_valid': acc_valid,\n",
    "                              'rmse_test': rmse_test, 'acc_test': acc_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d7c1a2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seeds</th>\n",
       "      <th>rmse_train</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>rmse_valid</th>\n",
       "      <th>acc_valid</th>\n",
       "      <th>rmse_test</th>\n",
       "      <th>acc_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>0.330292</td>\n",
       "      <td>0.762041</td>\n",
       "      <td>0.447634</td>\n",
       "      <td>0.683010</td>\n",
       "      <td>0.459753</td>\n",
       "      <td>0.684083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.297547</td>\n",
       "      <td>0.787063</td>\n",
       "      <td>0.444561</td>\n",
       "      <td>0.691420</td>\n",
       "      <td>0.443327</td>\n",
       "      <td>0.686052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123</td>\n",
       "      <td>0.288196</td>\n",
       "      <td>0.789598</td>\n",
       "      <td>0.446774</td>\n",
       "      <td>0.686410</td>\n",
       "      <td>0.441364</td>\n",
       "      <td>0.686678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.302909</td>\n",
       "      <td>0.778384</td>\n",
       "      <td>0.443804</td>\n",
       "      <td>0.690346</td>\n",
       "      <td>0.443304</td>\n",
       "      <td>0.683815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>666</td>\n",
       "      <td>0.304352</td>\n",
       "      <td>0.777281</td>\n",
       "      <td>0.446395</td>\n",
       "      <td>0.687036</td>\n",
       "      <td>0.443564</td>\n",
       "      <td>0.695535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.297945</td>\n",
       "      <td>0.784230</td>\n",
       "      <td>0.444988</td>\n",
       "      <td>0.685694</td>\n",
       "      <td>0.436806</td>\n",
       "      <td>0.691957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>0.291909</td>\n",
       "      <td>0.788971</td>\n",
       "      <td>0.447244</td>\n",
       "      <td>0.695893</td>\n",
       "      <td>0.440301</td>\n",
       "      <td>0.694820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1993</td>\n",
       "      <td>0.295714</td>\n",
       "      <td>0.786556</td>\n",
       "      <td>0.441702</td>\n",
       "      <td>0.693030</td>\n",
       "      <td>0.442816</td>\n",
       "      <td>0.692494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13</td>\n",
       "      <td>0.293923</td>\n",
       "      <td>0.787480</td>\n",
       "      <td>0.443887</td>\n",
       "      <td>0.690257</td>\n",
       "      <td>0.442209</td>\n",
       "      <td>0.690525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>0.307481</td>\n",
       "      <td>0.774477</td>\n",
       "      <td>0.446066</td>\n",
       "      <td>0.686052</td>\n",
       "      <td>0.457594</td>\n",
       "      <td>0.680862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seeds  rmse_train  acc_train  rmse_valid  acc_valid  rmse_test  acc_test\n",
       "0     24    0.330292   0.762041    0.447634   0.683010   0.459753  0.684083\n",
       "1      3    0.297547   0.787063    0.444561   0.691420   0.443327  0.686052\n",
       "2    123    0.288196   0.789598    0.446774   0.686410   0.441364  0.686678\n",
       "3      7    0.302909   0.778384    0.443804   0.690346   0.443304  0.683815\n",
       "4    666    0.304352   0.777281    0.446395   0.687036   0.443564  0.695535\n",
       "5      5    0.297945   0.784230    0.444988   0.685694   0.436806  0.691957\n",
       "6     10    0.291909   0.788971    0.447244   0.695893   0.440301  0.694820\n",
       "7   1993    0.295714   0.786556    0.441702   0.693030   0.442816  0.692494\n",
       "8     13    0.293923   0.787480    0.443887   0.690257   0.442209  0.690525\n",
       "9     12    0.307481   0.774477    0.446066   0.686052   0.457594  0.680862"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "55c4b2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seeds         3.000000\n",
       "rmse_train    0.288196\n",
       "acc_train     0.762041\n",
       "rmse_valid    0.441702\n",
       "acc_valid     0.683010\n",
       "rmse_test     0.436806\n",
       "acc_test      0.680862\n",
       "dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_errors.min(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "21fc589c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seeds         1993.000000\n",
       "rmse_train       0.330292\n",
       "acc_train        0.789598\n",
       "rmse_valid       0.447634\n",
       "acc_valid        0.695893\n",
       "rmse_test        0.459753\n",
       "acc_test         0.695535\n",
       "dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_errors.max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e849c115",
   "metadata": {},
   "source": [
    "save all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "934ed435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_errors.to_csv('as_paper_results_errors_reg.csv', index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cb5fb2",
   "metadata": {},
   "source": [
    "Now, also add results from the oiginal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "83eaa164",
   "metadata": {},
   "outputs": [],
   "source": [
    "as_paper_results_errors_reg = pd.read_csv(\"as_paper_results_errors_reg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "156d0087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seeds</th>\n",
       "      <th>rmse_train</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>rmse_valid</th>\n",
       "      <th>acc_valid</th>\n",
       "      <th>rmse_test</th>\n",
       "      <th>acc_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>0.330292</td>\n",
       "      <td>0.762041</td>\n",
       "      <td>0.447634</td>\n",
       "      <td>0.683010</td>\n",
       "      <td>0.459753</td>\n",
       "      <td>0.684083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.297547</td>\n",
       "      <td>0.787063</td>\n",
       "      <td>0.444561</td>\n",
       "      <td>0.691420</td>\n",
       "      <td>0.443327</td>\n",
       "      <td>0.686052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123</td>\n",
       "      <td>0.288196</td>\n",
       "      <td>0.789598</td>\n",
       "      <td>0.446774</td>\n",
       "      <td>0.686410</td>\n",
       "      <td>0.441364</td>\n",
       "      <td>0.686678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.302909</td>\n",
       "      <td>0.778384</td>\n",
       "      <td>0.443804</td>\n",
       "      <td>0.690346</td>\n",
       "      <td>0.443304</td>\n",
       "      <td>0.683815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>666</td>\n",
       "      <td>0.304352</td>\n",
       "      <td>0.777281</td>\n",
       "      <td>0.446395</td>\n",
       "      <td>0.687036</td>\n",
       "      <td>0.443564</td>\n",
       "      <td>0.695535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.297945</td>\n",
       "      <td>0.784230</td>\n",
       "      <td>0.444988</td>\n",
       "      <td>0.685694</td>\n",
       "      <td>0.436806</td>\n",
       "      <td>0.691957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>0.291909</td>\n",
       "      <td>0.788971</td>\n",
       "      <td>0.447244</td>\n",
       "      <td>0.695893</td>\n",
       "      <td>0.440301</td>\n",
       "      <td>0.694820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1993</td>\n",
       "      <td>0.295714</td>\n",
       "      <td>0.786556</td>\n",
       "      <td>0.441702</td>\n",
       "      <td>0.693030</td>\n",
       "      <td>0.442816</td>\n",
       "      <td>0.692494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13</td>\n",
       "      <td>0.293923</td>\n",
       "      <td>0.787480</td>\n",
       "      <td>0.443887</td>\n",
       "      <td>0.690257</td>\n",
       "      <td>0.442209</td>\n",
       "      <td>0.690525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>0.307481</td>\n",
       "      <td>0.774477</td>\n",
       "      <td>0.446066</td>\n",
       "      <td>0.686052</td>\n",
       "      <td>0.457594</td>\n",
       "      <td>0.680862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seeds  rmse_train  acc_train  rmse_valid  acc_valid  rmse_test  acc_test\n",
       "0     24    0.330292   0.762041    0.447634   0.683010   0.459753  0.684083\n",
       "1      3    0.297547   0.787063    0.444561   0.691420   0.443327  0.686052\n",
       "2    123    0.288196   0.789598    0.446774   0.686410   0.441364  0.686678\n",
       "3      7    0.302909   0.778384    0.443804   0.690346   0.443304  0.683815\n",
       "4    666    0.304352   0.777281    0.446395   0.687036   0.443564  0.695535\n",
       "5      5    0.297945   0.784230    0.444988   0.685694   0.436806  0.691957\n",
       "6     10    0.291909   0.788971    0.447244   0.695893   0.440301  0.694820\n",
       "7   1993    0.295714   0.786556    0.441702   0.693030   0.442816  0.692494\n",
       "8     13    0.293923   0.787480    0.443887   0.690257   0.442209  0.690525\n",
       "9     12    0.307481   0.774477    0.446066   0.686052   0.457594  0.680862"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "as_paper_results_errors_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b8ba7074",
   "metadata": {},
   "outputs": [],
   "source": [
    "as_paper_results_errors_reg = as_paper_results_errors_reg.append({'seeds': 42, 'rmse_train': 0.290209, 'acc_train': 0.789836,\n",
    "                                   'rmse_valid': 0.444209, 'acc_valid': 0.692494,\n",
    "                                   'rmse_test': 0.439565, 'acc_test': 0.699472}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3aa5b9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seeds</th>\n",
       "      <th>rmse_train</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>rmse_valid</th>\n",
       "      <th>acc_valid</th>\n",
       "      <th>rmse_test</th>\n",
       "      <th>acc_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0.330292</td>\n",
       "      <td>0.762041</td>\n",
       "      <td>0.447634</td>\n",
       "      <td>0.683010</td>\n",
       "      <td>0.459753</td>\n",
       "      <td>0.684083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.297547</td>\n",
       "      <td>0.787063</td>\n",
       "      <td>0.444561</td>\n",
       "      <td>0.691420</td>\n",
       "      <td>0.443327</td>\n",
       "      <td>0.686052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123.0</td>\n",
       "      <td>0.288196</td>\n",
       "      <td>0.789598</td>\n",
       "      <td>0.446774</td>\n",
       "      <td>0.686410</td>\n",
       "      <td>0.441364</td>\n",
       "      <td>0.686678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.302909</td>\n",
       "      <td>0.778384</td>\n",
       "      <td>0.443804</td>\n",
       "      <td>0.690346</td>\n",
       "      <td>0.443304</td>\n",
       "      <td>0.683815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>666.0</td>\n",
       "      <td>0.304352</td>\n",
       "      <td>0.777281</td>\n",
       "      <td>0.446395</td>\n",
       "      <td>0.687036</td>\n",
       "      <td>0.443564</td>\n",
       "      <td>0.695535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.297945</td>\n",
       "      <td>0.784230</td>\n",
       "      <td>0.444988</td>\n",
       "      <td>0.685694</td>\n",
       "      <td>0.436806</td>\n",
       "      <td>0.691957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.291909</td>\n",
       "      <td>0.788971</td>\n",
       "      <td>0.447244</td>\n",
       "      <td>0.695893</td>\n",
       "      <td>0.440301</td>\n",
       "      <td>0.694820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1993.0</td>\n",
       "      <td>0.295714</td>\n",
       "      <td>0.786556</td>\n",
       "      <td>0.441702</td>\n",
       "      <td>0.693030</td>\n",
       "      <td>0.442816</td>\n",
       "      <td>0.692494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.293923</td>\n",
       "      <td>0.787480</td>\n",
       "      <td>0.443887</td>\n",
       "      <td>0.690257</td>\n",
       "      <td>0.442209</td>\n",
       "      <td>0.690525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.307481</td>\n",
       "      <td>0.774477</td>\n",
       "      <td>0.446066</td>\n",
       "      <td>0.686052</td>\n",
       "      <td>0.457594</td>\n",
       "      <td>0.680862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>42.0</td>\n",
       "      <td>0.290209</td>\n",
       "      <td>0.789836</td>\n",
       "      <td>0.444209</td>\n",
       "      <td>0.692494</td>\n",
       "      <td>0.439565</td>\n",
       "      <td>0.699472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     seeds  rmse_train  acc_train  rmse_valid  acc_valid  rmse_test  acc_test\n",
       "0     24.0    0.330292   0.762041    0.447634   0.683010   0.459753  0.684083\n",
       "1      3.0    0.297547   0.787063    0.444561   0.691420   0.443327  0.686052\n",
       "2    123.0    0.288196   0.789598    0.446774   0.686410   0.441364  0.686678\n",
       "3      7.0    0.302909   0.778384    0.443804   0.690346   0.443304  0.683815\n",
       "4    666.0    0.304352   0.777281    0.446395   0.687036   0.443564  0.695535\n",
       "5      5.0    0.297945   0.784230    0.444988   0.685694   0.436806  0.691957\n",
       "6     10.0    0.291909   0.788971    0.447244   0.695893   0.440301  0.694820\n",
       "7   1993.0    0.295714   0.786556    0.441702   0.693030   0.442816  0.692494\n",
       "8     13.0    0.293923   0.787480    0.443887   0.690257   0.442209  0.690525\n",
       "9     12.0    0.307481   0.774477    0.446066   0.686052   0.457594  0.680862\n",
       "10    42.0    0.290209   0.789836    0.444209   0.692494   0.439565  0.699472"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "as_paper_results_errors_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2b916411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxplot = as_paper_results_errors_reg.boxplot(column=[\"rmse_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7a2ac328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxplot = as_paper_results_errors_reg.boxplot(column=[\"acc_test\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
